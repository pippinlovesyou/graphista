
================================================================================
FILE: graphrouter/README.md
================================================================================
# GraphRouter

A flexible Python graph database router library that provides a unified interface for working with multiple graph database backends.

[![PyPI version](https://badge.fury.io/py/graphrouter.svg)](https://badge.fury.io/py/graphrouter)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python Versions](https://img.shields.io/pypi/pyversions/graphrouter.svg)](https://pypi.org/project/graphrouter/)
[![Test Coverage](https://img.shields.io/codecov/c/github/graphrouter/graphrouter)](https://codecov.io/gh/graphrouter/graphrouter)

## Features

- ðŸ”„ **Multiple Backend Support**: Seamlessly work with different graph databases (Neo4j, FalkorDB, Local JSON)
- ðŸ” **Unified Query Interface**: Write queries once, run them on any supported backend
- ðŸ“Š **Rich Query Capabilities**: Support for complex graph traversals, aggregations, and pattern matching
- ðŸ”’ **Schema Validation**: Built-in ontology management for data validation
- ðŸ’¼ **Transaction Management**: ACID compliance with transaction support
- ðŸš€ **Performance Features**: 
  - Advanced query caching with pattern-based invalidation
  - Connection pooling for efficient resource utilization
  - Comprehensive performance monitoring and metrics
- ðŸ“ **Type Safety**: Full type hints support for better IDE integration

## Installation

```bash
pip install graphrouter
```

## Quick Start

```python
from graphrouter import LocalGraphDatabase, Query

# Initialize database
db = LocalGraphDatabase()
db.connect(db_path="graph.json")

# Create nodes
alice = db.create_node('Person', {'name': 'Alice', 'age': 30})
bob = db.create_node('Person', {'name': 'Bob', 'age': 25})

# Create relationship
db.create_edge(alice, bob, 'FRIENDS_WITH', {'since': '2023'})

# Query with caching enabled
query = Query()
query.filter(Query.label_equals('Person'))
query.filter(Query.property_equals('age', 30))
results = db.query(query)

# Check performance metrics
metrics = db.get_performance_metrics()
print(f"Average query time: {metrics['query']:.3f}s")
```

For more information, please see the main project [README](../README.md).
================================================================================


================================================================================
FILE: graphrouter/__init__.py
================================================================================
"""
GraphRouter - A flexible graph database router with multiple backend support.
"""

from .base import GraphDatabase
from .local import LocalGraphDatabase
from .neo4j import Neo4jGraphDatabase
from .falkordb import FalkorDBGraphDatabase
from .ontology import Ontology
from .query import Query
from .errors import (
    GraphRouterError,
    ConnectionError,
    QueryError,
    OntologyError
)

__version__ = "0.1.0"
__all__ = [
    'GraphDatabase',
    'LocalGraphDatabase',
    'Neo4jGraphDatabase',
    'FalkorDBGraphDatabase',
    'Ontology',
    'Query',
    'GraphRouterError',
    'ConnectionError',
    'QueryError',
    'OntologyError'
]
================================================================================


================================================================================
FILE: graphrouter/base.py
================================================================================
"""
Base abstract classes for graph database implementations.
"""
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import time as _time

from .ontology import Ontology
from .query import Query
from .cache import QueryCache
from .monitoring import PerformanceMonitor
from .errors import ConnectionError, InvalidNodeTypeError, InvalidPropertyError


class GraphDatabase(ABC):
    """Abstract base class for graph database implementations."""

    def __init__(self, pool_size: int = 5):
        self.ontology: Optional[Ontology] = None
        self.connected: bool = False
        self._pool_size = pool_size
        self._connection_pool = []
        self._cache = QueryCache()
        self._monitor = PerformanceMonitor()

    #
    # Connection
    #
    @abstractmethod
    def connect(self, **kwargs) -> bool:
        """Connect to the database."""
        pass

    @abstractmethod
    def disconnect(self) -> bool:
        """Disconnect from the database."""
        pass

    #
    # NODE OPERATIONS
    #
    def create_node(self, label: str, properties: Dict[str, Any] = None) -> str:
        """Create a new node with the given label and properties."""
        if not self.connected:
            raise ConnectionError("Database not connected")

        if properties is None:
            raise ValueError("Properties cannot be None")

        # Optional: map and then validate at the ontology level
        if self.ontology:
            properties = self.ontology.map_node_properties(label, properties)

        # High-level validation that returns True/False
        if not self.validate_node(label, properties):
            raise ValueError("Node validation failed")

        start_time = _time.perf_counter()
        node_id = self._create_node_impl(label, properties)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("create_node", duration)
        return node_id

    @abstractmethod
    def _create_node_impl(self, label: str, properties: Dict[str, Any]) -> str:
        pass

    def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        if not self.connected:
            raise ConnectionError("Database not connected")

        cache_key = f"node:{node_id}"
        cached_result = self._cache.get(cache_key)
        if cached_result is not None:
            return cached_result

        start_time = _time.perf_counter()
        node = self._get_node_impl(node_id)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("get_node", duration)
        if node is not None:
            self._cache.set(cache_key, node)
        return node

    @abstractmethod
    def _get_node_impl(self, node_id: str) -> Optional[Dict[str, Any]]:
        pass

    def update_node(self, node_id: str, properties: Dict[str, Any]) -> bool:
        if not self.connected:
            raise ConnectionError("Database not connected")

        if properties is None:
            raise ValueError("Properties cannot be None")

        start_time = _time.perf_counter()

        current = self._get_node_impl(node_id)
        if not current:
            return False

        updated_props = {**current["properties"], **properties}
        if not self.validate_node(current["label"], updated_props):
            raise ValueError("Node validation failed")

        success = self._update_node_impl(node_id, properties)

        duration = _time.perf_counter() - start_time
        self._monitor.record_operation("update_node", duration)
        self._cache.invalidate(f"node:{node_id}")

        return success

    @abstractmethod
    def _update_node_impl(self, node_id: str, properties: Dict[str, Any]) -> bool:
        pass

    def delete_node(self, node_id: str) -> bool:
        if not self.connected:
            raise ConnectionError("Database not connected")

        start_time = _time.perf_counter()
        success = self._delete_node_impl(node_id)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("delete_node", duration)
        self._cache.invalidate(f"node:{node_id}")

        return success

    @abstractmethod
    def _delete_node_impl(self, node_id: str) -> bool:
        pass

    #
    # EDGE OPERATIONS
    #
    def create_edge(self, from_id: str, to_id: str, label: str, properties: Optional[Dict[str, Any]] = None) -> str:
        if not self.connected:
            raise ConnectionError("Database not connected")

        if properties is None:
            raise ValueError("Properties cannot be None")

        # If using ontology
        if self.ontology:
            properties = self.ontology.map_edge_properties(label, properties)

        if not self.validate_edge(label, properties):
            raise ValueError("Edge validation failed")

        start_time = _time.perf_counter()
        edge_id = self._create_edge_impl(from_id, to_id, label, properties)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("create_edge", duration)
        return edge_id

    @abstractmethod
    def _create_edge_impl(self, from_id: str, to_id: str, label: str, properties: Dict[str, Any]) -> str:
        pass

    def get_edge(self, edge_id: str) -> Optional[Dict[str, Any]]:
        if not self.connected:
            raise ConnectionError("Database not connected")

        cache_key = f"edge:{edge_id}"
        cached_result = self._cache.get(cache_key)
        if cached_result is not None:
            return cached_result

        start_time = _time.perf_counter()
        edge = self._get_edge_impl(edge_id)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("get_edge", duration)
        if edge is not None:
            self._cache.set(cache_key, edge)
        return edge

    @abstractmethod
    def _get_edge_impl(self, edge_id: str) -> Optional[Dict[str, Any]]:
        pass

    def update_edge(self, edge_id: str, properties: Dict[str, Any]) -> bool:
        if not self.connected:
            raise ConnectionError("Database not connected")

        if properties is None:
            raise ValueError("Properties cannot be None")

        start_time = _time.perf_counter()

        current = self._get_edge_impl(edge_id)
        if not current:
            return False

        updated_props = {**current["properties"], **properties}
        if not self.validate_edge(current["label"], updated_props):
            raise ValueError("Edge validation failed")

        success = self._update_edge_impl(edge_id, properties)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("update_edge", duration)
        self._cache.invalidate(f"edge:{edge_id}")

        return success

    @abstractmethod
    def _update_edge_impl(self, edge_id: str, properties: Dict[str, Any]) -> bool:
        pass

    def delete_edge(self, edge_id: str) -> bool:
        if not self.connected:
            raise ConnectionError("Database not connected")

        start_time = _time.perf_counter()
        success = self._delete_edge_impl(edge_id)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("delete_edge", duration)
        self._cache.invalidate(f"edge:{edge_id}")

        return success

    @abstractmethod
    def _delete_edge_impl(self, edge_id: str) -> bool:
        pass

    #
    # QUERY
    #
    def query(self, query: Query) -> List[Dict[str, Any]]:
        if not self.connected:
            raise ConnectionError("Database not connected")

        cache_key = f"query:{hash(str(query))}"
        cached = self._cache.get(cache_key)
        if cached is not None:
            return cached

        start_time = _time.perf_counter()
        results = self._query_impl(query)
        duration = _time.perf_counter() - start_time

        self._monitor.record_operation("query", duration)
        self._cache.set(cache_key, results)

        query._execution_time = duration
        query._memory_used = float(len(str(results)))
        return results

    @abstractmethod
    def _query_impl(self, query: Query) -> List[Dict[str, Any]]:
        pass

    #
    # BATCH OPERATIONS
    #
    def batch_create_nodes(self, nodes: List[Dict[str, Any]]) -> List[str]:
        if not self.connected:
            raise ConnectionError("Database not connected")

        start_time = _time.perf_counter()

        for node in nodes:
            if 'label' not in node or 'properties' not in node:
                raise ValueError("Invalid node format")
            if node['properties'] is None:
                raise ValueError("Properties cannot be None")
            if not self.validate_node(node['label'], node['properties']):
                raise ValueError(f"Node validation failed for node: {node}")

        node_ids = self._batch_create_nodes_impl(nodes)
        duration = _time.perf_counter() - start_time
        self._monitor.record_operation("batch_create_nodes", duration)
        return node_ids

    @abstractmethod
    def _batch_create_nodes_impl(self, nodes: List[Dict[str, Any]]) -> List[str]:
        pass

    def batch_create_edges(self, edges: List[Dict[str, Any]]) -> List[str]:
        if not self.connected:
            raise ConnectionError("Database not connected")

        start_time = _time.perf_counter()

        for edge in edges:
            if not all(k in edge for k in ['from_id', 'to_id', 'label']):
                raise ValueError("Invalid edge format")
            props = edge.get('properties', {})
            if props is None:
                raise ValueError("Properties cannot be None")
            if not self.validate_edge(edge['label'], props):
                raise ValueError(f"Edge validation failed for edge: {edge}")

        edge_ids = self._batch_create_edges_impl(edges)
        duration = _time.perf_counter() - start_time
        self._monitor.record_operation("batch_create_edges", duration)
        return edge_ids

    @abstractmethod
    def _batch_create_edges_impl(self, edges: List[Dict[str, Any]]) -> List[str]:
        pass

    #
    # ONTOLOGY / VALIDATION
    #
    def set_ontology(self, ontology: Ontology):
        self.ontology = ontology

    def validate_node(self, label: str, properties: Dict[str, Any]) -> bool:
        """Return True if node is valid, else False."""
        if not self.ontology:
            return True
        try:
            self.ontology.validate_node(label, properties)
            return True
        except (InvalidNodeTypeError, InvalidPropertyError):
            return False

    def validate_edge(self, label: str, properties: Dict[str, Any]) -> bool:
        """Return True if edge is valid, else False."""
        if not self.ontology:
            return True
        try:
            self.ontology.validate_edge(label, properties)
            return True
        except (InvalidNodeTypeError, InvalidPropertyError):
            return False

    #
    # MONITORING / CACHE
    #
    def get_performance_metrics(self) -> Dict[str, float]:
        return self._monitor.get_average_times()

    def reset_metrics(self):
        self._monitor.reset()

    def clear_cache(self):
        self._cache = QueryCache()

================================================================================


================================================================================
FILE: graphrouter/cache.py
================================================================================
"""Cache management for GraphRouter."""
from typing import Any, Dict, Optional, Set
from datetime import datetime, timedelta
import time

class QueryCache:
    def __init__(self, ttl: int = 300):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.ttl = ttl
        self.invalidation_patterns: Dict[str, Set[str]] = {}

    def get(self, key: str) -> Optional[Any]:
        """Retrieve a value from cache if it exists and hasn't expired."""
        if key in self.cache:
            entry = self.cache[key]
            if datetime.now() - entry['timestamp'] < timedelta(seconds=self.ttl):
                return entry['data']
            else:
                # Remove expired entry
                del self.cache[key]
        return None

    def set(self, key: str, value: Any):
        """Store a value in cache with current timestamp."""
        self.cache[key] = {
            'data': value,
            'timestamp': datetime.now()
        }

        # Register for pattern-based invalidation
        parts = key.split(':')
        if len(parts) > 1:
            pattern = f"{parts[0]}:*"
            if pattern not in self.invalidation_patterns:
                self.invalidation_patterns[pattern] = set()
            self.invalidation_patterns[pattern].add(key)

    def invalidate(self, pattern: str):
        """Invalidate cache entries matching the given pattern."""
        if pattern in self.invalidation_patterns:
            for key in self.invalidation_patterns[pattern]:
                if key in self.cache:
                    del self.cache[key]
            del self.invalidation_patterns[pattern]

    def clear(self):
        """Clear all cache entries."""
        self.cache.clear()
        self.invalidation_patterns.clear()

    def cleanup(self):
        """Remove expired entries from cache."""
        now = datetime.now()
        expired_keys = [
            key for key, entry in self.cache.items()
            if now - entry['timestamp'] >= timedelta(seconds=self.ttl)
        ]
        for key in expired_keys:
            del self.cache[key]
            # Clean up invalidation patterns
            for pattern, keys in list(self.invalidation_patterns.items()):
                keys.discard(key)
                if not keys:
                    del self.invalidation_patterns[pattern]
================================================================================


================================================================================
FILE: graphrouter/config.py
================================================================================
"""Configuration management for GraphRouter."""
import os
from typing import Optional, Dict, Any, cast
from pathlib import Path

class Config:
    """Configuration handler for GraphRouter."""

    @staticmethod
    def get_env(key: str, default: Optional[str] = None) -> Optional[str]:
        """Get environment variable from either Replit secrets or .env file.

        Args:
            key: Environment variable key
            default: Default value if key is not found

        Returns:
            str: Value of environment variable or default

        Note:
            Will check Replit secrets first, then fall back to .env file
        """
        # First try Replit secrets
        value = os.environ.get(key)
        if value is not None:
            return value

        # Then try .env file
        env_path = Path('.env')
        if env_path.exists():
            with env_path.open() as f:
                for line in f:
                    if '=' in line:
                        k, v = line.strip().split('=', 1)
                        if k == key:
                            return v.strip('"\'')

        return default

    @staticmethod
    def get_int_env(key: str, default: int) -> int:
        """Get integer environment variable with proper type casting.

        Args:
            key: Environment variable key
            default: Default value if key is not found or invalid

        Returns:
            int: Value of environment variable or default
        """
        value = Config.get_env(key)
        try:
            return int(value) if value is not None else default
        except (ValueError, TypeError):
            return default

    @staticmethod
    def get_falkordb_config() -> Dict[str, Any]:
        """Get FalkorDB configuration from environment.

        Returns:
            Dict containing FalkorDB connection configuration

        Note:
            Will set sensible defaults for non-critical parameters
        """
        return {
            'host': Config.get_env('FALKORDB_HOST', 'localhost'),
            'port': Config.get_int_env('FALKORDB_PORT', 6379),
            'username': Config.get_env('FALKORDB_USERNAME'),
            'password': Config.get_env('FALKORDB_PASSWORD'),
            'graph_name': Config.get_env('FALKORDB_GRAPH', 'graph')
        }
================================================================================


================================================================================
FILE: graphrouter/core_ontology.py
================================================================================

"""Core ontology definitions for GraphRouter."""
from typing import Dict, Any, Union
from .ontology import Ontology

def create_core_ontology() -> Ontology:
    """Create and return the core system ontology."""
    ontology = Ontology()
    
    # Core data types
    ontology.add_node_type(
        "DataSource",
        {"name": "str", "type": "str"},
        ["name"]
    )
    
    ontology.add_node_type(
        "File",
        {
            "file_name": "str",
            "path": "str",
            "uploaded_time": "float",
            "mime_type": "str"
        },
        ["file_name", "path"]
    )
    
    ontology.add_node_type(
        "Row",
        {"raw_data": "str"},
        ["raw_data"]
    )
    
    ontology.add_node_type(
        "Log",
        {
            "timestamp": "float",
            "type": "str",
            "message": "str"
        },
        ["timestamp", "type"]
    )
    
    ontology.add_node_type(
        "SearchResult",
        {
            "content": "str",
            "query_string": "str",
            "score": "float"
        },
        ["content", "query_string"]
    )
    
    ontology.add_node_type(
        "Row",
        {
            "raw_data": "str",
            "id": "str",
            "name": "str"
        },
        []  # No required fields for CSV rows
    )
    
    ontology.add_node_type(
        "Log",
        {
            "timestamp": "float",
            "type": "str",
            "message": "str",
            "data": "str",
            "action": "str",
            "params": "str",
            "result": "str",
            "details": "str",
            "data_source": "str"
        },
        ["timestamp"]  # Only timestamp is required
    )
    
    ontology.add_node_type(
        "Webhook",
        {
            "event": "str",
            "payload": "str",
            "timestamp": "float"
        },
        ["event", "timestamp"]
    )
    
    # Core relationships
    ontology.add_edge_type(
        "HAS_FILE",
        {"timestamp": "float"},
        []  # Make timestamp optional
    )
    
    ontology.add_edge_type(
        "HAS_ROW",
        {"row_number": "int"},
        []  # Make row_number optional
    )
    
    ontology.add_edge_type(
        "HAS_LOG",
        {"timestamp": "float"},
        []  # Make timestamp optional
    )
    
    ontology.add_edge_type(
        "HAS_WEBHOOK",
        {"timestamp": "float"},
        []  # Add webhook relationship
    )
    
    ontology.add_edge_type(
        "HAS_SYNC",
        {"timestamp": "float"},
        []  # Add sync relationship
    )
    
    return ontology

def extend_ontology(base_ontology: Ontology, extensions: Union[Dict[str, Any], Ontology]) -> Ontology:
    """Extend the core ontology with custom types."""
    if isinstance(extensions, Ontology):
        # If extensions is an Ontology, merge its types directly
        for node_type, spec in extensions.node_types.items():
            base_ontology.add_node_type(
                node_type,
                spec['properties'],
                spec['required']
            )
        
        for edge_type, spec in extensions.edge_types.items():
            base_ontology.add_edge_type(
                edge_type,
                spec['properties'],
                spec['required']
            )
    else:
        # Handle dictionary case
        for node_type, spec in extensions.get('node_types', {}).items():
            base_ontology.add_node_type(
                node_type,
                spec.get('properties', {}),
                spec.get('required', [])
            )
        
        for edge_type, spec in extensions.get('edge_types', {}).items():
            base_ontology.add_edge_type(
                edge_type,
                spec.get('properties', {}),
                spec.get('required', [])
            )
    
    return base_ontology

================================================================================


================================================================================
FILE: graphrouter/errors.py
================================================================================
"""
Custom exceptions for the GraphRouter library.
"""

class GraphRouterError(Exception):
    """Base exception for all GraphRouter errors."""
    pass

class ConnectionError(GraphRouterError):
    """Raised when there are issues connecting to the database."""
    pass

class QueryError(GraphRouterError):
    """Raised when there are issues with query execution."""
    pass

class OntologyError(GraphRouterError):
    """Base class for ontology-related errors."""
    def __init__(self, message: str, available_options: dict = None):
        self.available_options = available_options
        super().__init__(message)

class InvalidNodeTypeError(OntologyError):
    """Raised when an invalid node type is used."""
    pass

class InvalidPropertyError(OntologyError):
    """Raised when invalid properties are provided."""
    pass

================================================================================


================================================================================
FILE: graphrouter/falkordb.py
================================================================================
# falkordb.py

import ast
import re
from typing import Dict, List, Any, Optional, cast
from redis import Redis, ConnectionPool

from .base import GraphDatabase
from .errors import ConnectionError
from .query import Query
from .config import Config


class FalkorDBGraphDatabase(GraphDatabase):
    """FalkorDB graph database implementation using RedisGraph."""

    def __init__(self, pool_size: int = 5):
        super().__init__(pool_size=pool_size)
        self.client: Optional[Redis] = None
        self.graph_name: str = "graph"
        self.pool: Optional[ConnectionPool] = None

    def connect(self, **kwargs) -> bool:
        """Connect to FalkorDB (RedisGraph)."""
        try:
            config = Config.get_falkordb_config()
            config.update(kwargs)

            self.pool = ConnectionPool(
                host=config['host'],
                port=config['port'],
                username=config['username'],
                password=config['password'],
                decode_responses=True
            )
            self.client = Redis(connection_pool=self.pool)
            self.graph_name = config.get('graph_name', 'graph')

            # Test connection
            self.client.ping()
            self.connected = True
            return True
        except Exception as e:
            raise ConnectionError(f"Failed to connect to FalkorDB: {str(e)}")

    def disconnect(self) -> bool:
        """Disconnect from FalkorDB (RedisGraph)."""
        if self.client:
            self.client.close()
            self.client = None
        if self.pool:
            self.pool.disconnect()
        self.connected = False
        return True

    def _execute_graph_query(self, query_str: str, *args) -> Any:
        """
        Execute a RedisGraph query, rewriting "CONTAINS(...)" to a regex match 
        that RedisGraph understands, and also specifying "COMPACT" for a simpler response.
        """
        if not self.client:
            raise ConnectionError("Database not connected")

        # Rewrites "CONTAINS(n.someProp, 'val')" -> "n.someProp =~ '.*val.*'"
        pattern = r"CONTAINS\(n\.(\w+),\s*'([^']*)'\)"
        repl = r"n.\1 =~ '.*\2.*'"
        query_str = re.sub(pattern, repl, query_str)

        client = cast(Redis, self.client)
        # Add "COMPACT" so we get simpler row data
        return client.execute_command("GRAPH.QUERY", self.graph_name, query_str, "COMPACT", *args)

    def _parse_properties(self, props: Any) -> Dict[str, Any]:
        """
        Convert the result of 'properties(n)' to a Python dict.
        If it's a node dictionary or string, parse carefully.
        """
        if not props:
            return {}
        if isinstance(props, dict):
            return props
        if not isinstance(props, str):
            return {}

        try:
            val = ast.literal_eval(props)
            if isinstance(val, dict):
                return val
        except Exception:
            pass

        # Fallback parse: "k=v" or "k:v"
        result: Dict[str, Any] = {}
        tokens = re.split(r"[,\s]+", props.strip())
        for tok in tokens:
            if '=' in tok:
                sep = '='
            elif ':' in tok:
                sep = ':'
            else:
                continue
            parts = tok.split(sep, 1)
            if len(parts) == 2:
                k = parts[0].strip()
                v = parts[1].strip().strip('"').strip("'")
                result[k] = v
        return result

    def _extract_id_from_cell(self, cell) -> int:
        """Safely extract a numeric ID from the cell, which might be string '42', int 42, or dict."""
        if isinstance(cell, int):
            return cell
        if isinstance(cell, str) and cell.isdigit():
            return int(cell)
        if isinstance(cell, dict) and "id" in cell:
            return int(cell["id"])
        raise ValueError(f"Unexpected row cell for ID: {cell!r}")

    #
    # NODE IMPLEMENTATION
    #
    def _create_node_impl(self, label: str, properties: Dict[str, Any]) -> int:
        if not self.validate_node(label, properties):
            raise ValueError("Node validation failed")

        props_copy = {}
        for k, v in properties.items():
            if isinstance(v, list):
                v = ",".join(str(x) for x in v)
            props_copy[k] = v

        props_str = ", ".join(
            f"{k}: '{val}'" if isinstance(val, str) else f"{k}: {val}"
            for k, val in props_copy.items()
        )
        cypher = f"CREATE (n:{label} {{{props_str}}}) RETURN ID(n)"
        raw = self._execute_graph_query(cypher)
        if not raw or len(raw) < 2 or not raw[1]:
            raise ValueError("Failed to create node in RedisGraph (no rows)")
        first_row = raw[1][0]
        if not first_row:
            raise ValueError("Failed to create node (empty row)")

        return self._extract_id_from_cell(first_row[0])

    def _get_node_impl(self, node_id: str) -> Optional[Dict[str, Any]]:
        try:
            node_id_int = int(node_id)
        except ValueError:
            return None
        cypher = f"MATCH (n) WHERE ID(n) = {node_id_int} RETURN n"
        raw = self._execute_graph_query(cypher)
        if not raw or len(raw) < 2 or not raw[1]:
            return None
        row = raw[1][0]
        if not row:
            return None
        cell = row[0]
        if isinstance(cell, dict):
            return {
                "id": str(cell.get("id", "")),
                "label": cell.get("labels", [None])[0],
                "properties": cell.get("properties", {})
            }
        return None

    def _update_node_impl(self, node_id: str, properties: Dict[str, Any]) -> bool:
        curr = self._get_node_impl(node_id)
        if not curr:
            return False

        updated_props = {**curr["properties"], **properties}
        if not self.validate_node(curr["label"], updated_props):
            raise ValueError("Node validation failed")

        props_copy = {}
        for k, v in properties.items():
            if isinstance(v, list):
                v = ",".join(str(x) for x in v)
            props_copy[k] = v

        set_expr = ", ".join(
            f"n.{k} = '{val}'" if isinstance(val, str) else f"n.{k} = {val}"
            for k, val in props_copy.items()
        )
        cypher = f"MATCH (n) WHERE ID(n) = {node_id} SET {set_expr} RETURN ID(n)"
        raw = self._execute_graph_query(cypher)
        if not raw or len(raw) < 2 or not raw[1]:
            return False
        return True

    def _delete_node_impl(self, node_id: str) -> bool:
        try:
            node_id_int = int(node_id)
        except ValueError:
            return False
        cypher = f"MATCH (n) WHERE ID(n) = {node_id_int} DETACH DELETE n"
        self._execute_graph_query(cypher)
        return True

    #
    # EDGE IMPLEMENTATION
    #
    def _create_edge_impl(self, from_id: str, to_id: str, label: str, properties: Dict[str, Any]) -> int:
        if not self._get_node_impl(from_id) or not self._get_node_impl(to_id):
            raise ValueError("One or both nodes do not exist")
        if not self.validate_edge(label, properties):
            raise ValueError("Edge validation failed")

        props_copy = {}
        for k, v in properties.items():
            if isinstance(v, list):
                v = ",".join(str(x) for x in v)
            props_copy[k] = v

        props_str = ""
        if props_copy:
            plist = [
                f"{kk}: '{vv}'" if isinstance(vv, str) else f"{kk}: {vv}"
                for kk, vv in props_copy.items()
            ]
            props_str = " {" + ", ".join(plist) + "}"

        cypher = (
            f"MATCH (a), (b) "
            f"WHERE ID(a) = {from_id} AND ID(b) = {to_id} "
            f"CREATE (a)-[r:{label}{props_str}]->(b) RETURN ID(r)"
        )
        raw = self._execute_graph_query(cypher)
        if not raw or len(raw) < 2 or not raw[1]:
            raise ValueError("Failed to create edge")
        first_row = raw[1][0]
        return self._extract_id_from_cell(first_row[0])

    def _get_edge_impl(self, edge_id: str) -> Optional[Dict[str, Any]]:
        try:
            edge_id_int = int(edge_id)
        except ValueError:
            return None
        cypher = f"MATCH ()-[r]->() WHERE ID(r) = {edge_id_int} RETURN r"
        raw = self._execute_graph_query(cypher)
        if not raw or len(raw) < 2 or not raw[1]:
            return None
        row = raw[1][0]
        if not row:
            return None
        cell = row[0]
        if isinstance(cell, dict):
            return {
                "label": cell.get("type", ""),
                "properties": cell.get("properties", {}),
                "from_id": str(cell.get("src_node", "")),
                "to_id": str(cell.get("dst_node", ""))
            }
        return None

    def _update_edge_impl(self, edge_id: str, properties: Dict[str, Any]) -> bool:
        curr = self._get_edge_impl(edge_id)
        if not curr:
            return False
        updated_props = {**curr["properties"], **properties}
        if not self.validate_edge(curr["label"], updated_props):
            raise ValueError("Edge validation failed")

        props_copy = {}
        for k, v in properties.items():
            if isinstance(v, list):
                v = ",".join(str(x) for x in v)
            props_copy[k] = v

        set_expr = ", ".join(
            f"r.{kk} = '{vv}'" if isinstance(vv, str) else f"r.{kk} = {vv}"
            for kk, vv in props_copy.items()
        )
        cypher = f"MATCH ()-[r]->() WHERE ID(r) = {edge_id} SET {set_expr} RETURN ID(r)"
        raw = self._execute_graph_query(cypher)
        if not raw or len(raw) < 2 or not raw[1]:
            return False
        return True

    def _delete_edge_impl(self, edge_id: str) -> bool:
        try:
            edge_id_int = int(edge_id)
        except ValueError:
            return False
        cypher = f"MATCH ()-[r]->() WHERE ID(r) = {edge_id_int} DELETE r"
        self._execute_graph_query(cypher)
        return True

    #
    # QUERY IMPLEMENTATION
    #
    def _build_cypher_query(self, query: Query) -> str:
        parts = ["MATCH (n)"]
        wheres = []
        for f in query.filters:
            if hasattr(f, "filter_type"):
                t = f.filter_type
                if t == "label_equals":
                    wheres.append(f"n:{f.label}")
                elif t == "property_equals":
                    wheres.append(f"n.{f.property_name} = {repr(f.value)}")
                elif t == "property_contains":
                    # produce EXACT "CONTAINS(n.X, 'Y')"
                    wheres.append(f"CONTAINS(n.{f.property_name}, {repr(f.value)})")

        if wheres:
            parts.append("WHERE " + " AND ".join(wheres))

        if query.sort_key:
            direction = "DESC" if query.sort_reverse else "ASC"
            parts.append(f"ORDER BY n.{query.sort_key} {direction}")

        if query.limit:
            parts.append(f"LIMIT {query.limit}")

        parts.append("RETURN n")
        return " ".join(parts)

    def _query_impl(self, query: Query) -> List[Dict[str, Any]]:
        cypher = self._build_cypher_query(query)
        raw = self._execute_graph_query(cypher)
        data_rows = raw[1] if raw and len(raw) > 1 else []

        results = []
        for record in data_rows:
            if not record:
                continue
            cell = record[0]
            if isinstance(cell, dict):
                results.append({
                    "id": str(cell.get("id", "")),
                    "label": cell.get("labels", [""])[0],
                    "properties": cell.get("properties", {})
                })
        return results

    #
    # BATCH IMPLEMENTATION
    #
    def _batch_create_nodes_impl(self, nodes: List[Dict[str, Any]]) -> List[str]:
        for n in nodes:
            if not self.validate_node(n["label"], n["properties"]):
                raise ValueError(f"Node validation failed for node: {n}")

        queries = []
        for node in nodes:
            props_copy = {}
            for k, v in node["properties"].items():
                if isinstance(v, list):
                    v = ",".join(str(x) for x in v)
                props_copy[k] = v

            props_str = ", ".join(
                f"{kk}: '{vv}'" if isinstance(vv, str) else f"{kk}: {vv}"
                for kk, vv in props_copy.items()
            )
            queries.append(f"CREATE (n:{node['label']} {{{props_str}}}) RETURN ID(n)")

        union_query = " UNION ALL ".join(queries)
        raw = self._execute_graph_query(union_query)
        if not raw or len(raw) < 2:
            return []

        data_rows = raw[1]
        node_ids: List[str] = []
        for row in data_rows:
            if not row:
                continue
            val = row[0]
            node_id_int = self._extract_id_from_cell(val)
            node_ids.append(str(node_id_int))
        return node_ids

    def _batch_create_edges_impl(self, edges: List[Dict[str, Any]]) -> List[str]:
        for e in edges:
            if not self.validate_edge(e["label"], e.get("properties", {})):
                raise ValueError(f"Edge validation failed for {e}")

        queries = []
        for edge in edges:
            from_id = edge["from_id"]
            to_id = edge["to_id"]
            props_copy = {}
            for k, v in edge.get("properties", {}).items():
                if isinstance(v, list):
                    v = ",".join(str(x) for x in v)
                props_copy[k] = v

            props_str = ""
            if props_copy:
                plist = [
                    f"{kk}: '{vv}'" if isinstance(vv, str) else f"{kk}: {vv}"
                    for kk, vv in props_copy.items()
                ]
                props_str = " {" + ", ".join(plist) + "}"

            queries.append(
                f"MATCH (a), (b) "
                f"WHERE ID(a) = {from_id} AND ID(b) = {to_id} "
                f"CREATE (a)-[r:{edge['label']}{props_str}]->(b) RETURN ID(r)"
            )

        union_query = " UNION ALL ".join(queries)
        raw = self._execute_graph_query(union_query)
        if not raw or len(raw) < 2:
            return []

        data_rows = raw[1]
        edge_ids: List[str] = []
        for row in data_rows:
            if not row:
                continue
            val = row[0]
            edge_id = self._extract_id_from_cell(val)
            edge_ids.append(str(edge_id))
        return edge_ids

================================================================================


================================================================================
FILE: graphrouter/local.py
================================================================================
"""
Local JSON-based graph database implementation.
"""
import json
import os
import uuid
import time
from typing import Dict, List, Any, Optional, Set, Tuple
from collections import defaultdict

from .base import GraphDatabase
from .errors import ConnectionError
from .query import Query, AggregationType, PathPattern


class LocalGraphDatabase(GraphDatabase):
    """Local JSON-based graph database implementation."""

    def __init__(self, pool_size: int = 5):
        super().__init__(pool_size=pool_size)
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self.edges: Dict[str, Dict[str, Any]] = {}
        self.db_path: Optional[str] = None
        # Store adjacency only in forward direction: from_id -> list of (edge_id, to_id)
        self._edge_index: Dict[str, List[Tuple[str, str]]] = defaultdict(list)

    def connect(self, db_path: str = "graph.json") -> bool:
        """Connect to local JSON database."""
        if os.path.exists(db_path):
            try:
                with open(db_path, 'r') as f:
                    data = json.load(f)
                    self.nodes = data.get('nodes', {})
                    self.edges = data.get('edges', {})
                    self._update_edge_index()
            except json.JSONDecodeError:
                raise ConnectionError(f"Invalid JSON in database file: {db_path}")

        self.db_path = db_path
        self.connected = True
        return True

    def disconnect(self) -> bool:
        """Disconnect and save the database."""
        if self.db_path:
            with open(self.db_path, 'w') as f:
                json.dump({'nodes': self.nodes, 'edges': self.edges}, f, indent=2)
        self.connected = False
        return True

    def _update_edge_index(self):
        """Rebuild adjacency index for forward edges only."""
        self._edge_index.clear()
        for edge_id, edge in self.edges.items():
            f_id = str(edge['from_id'])
            t_id = str(edge['to_id'])
            self._edge_index[f_id].append((edge_id, t_id))

    #
    # OVERRIDE get_node(...) => dictionary check
    #
    def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        if not self.connected:
            raise ConnectionError("Database not connected")
        node_id = str(node_id)
        if node_id not in self.nodes:
            self._cache.invalidate(f"node:{node_id}")
            return None

        return super().get_node(node_id)

    #
    # OVERRIDE get_edge(...) => dictionary check
    #
    def get_edge(self, edge_id: str) -> Optional[Dict[str, Any]]:
        if not self.connected:
            raise ConnectionError("Database not connected")
        edge_id = str(edge_id)
        if edge_id not in self.edges:
            self._cache.invalidate(f"edge:{edge_id}")
            return None

        return super().get_edge(edge_id)

    #
    # NODE IMPL
    #
    def _create_node_impl(self, label: str, properties: Dict[str, Any]) -> str:
        node_id = str(uuid.uuid4())
        self.nodes[node_id] = {
            'label': label,
            'properties': properties
        }
        return node_id

    def _get_node_impl(self, node_id: str) -> Optional[Dict[str, Any]]:
        return self.nodes.get(str(node_id))

    def _update_node_impl(self, node_id: str, properties: Dict[str, Any]) -> bool:
        node_id = str(node_id)
        if node_id not in self.nodes:
            return False
        node = self.nodes[node_id]
        node['properties'] = {**node['properties'], **properties}
        return True

    def _delete_node_impl(self, node_id: str) -> bool:
        node_id = str(node_id)
        if node_id not in self.nodes:
            return False

        # Remove connected edges
        edges_to_delete = []
        for e_id, e in list(self.edges.items()):
            if str(e['from_id']) == node_id or str(e['to_id']) == node_id:
                edges_to_delete.append(e_id)

        for e_id in edges_to_delete:
            self.edges.pop(e_id, None)
            self._cache.invalidate(f"edge:{e_id}")

        self.nodes.pop(node_id, None)
        self._update_edge_index()
        return True

    #
    # EDGE IMPL
    #
    def _create_edge_impl(self, from_id: str, to_id: str, label: str, properties: Dict[str, Any]) -> str:
        from_id = str(from_id)
        to_id = str(to_id)
        if from_id not in self.nodes or to_id not in self.nodes:
            raise ValueError("Source or target node does not exist")

        edge_id = str(uuid.uuid4())
        self.edges[edge_id] = {
            'from_id': from_id,
            'to_id': to_id,
            'label': label,
            'properties': properties
        }
        self._update_edge_index()
        return edge_id

    def _get_edge_impl(self, edge_id: str) -> Optional[Dict[str, Any]]:
        return self.edges.get(str(edge_id))

    def _update_edge_impl(self, edge_id: str, properties: Dict[str, Any]) -> bool:
        edge_id = str(edge_id)
        if edge_id not in self.edges:
            return False
        edge = self.edges[edge_id]
        edge['properties'] = {**edge['properties'], **properties}
        return True

    def _delete_edge_impl(self, edge_id: str) -> bool:
        edge_id = str(edge_id)
        if edge_id not in self.edges:
            return False
        self.edges.pop(edge_id, None)
        self._update_edge_index()
        return True

    #
    # QUERY IMPL
    #
    def _query_impl(self, query: Query) -> List[Dict[str, Any]]:
        time.sleep(0.0001)  # ensure a measurable query time

        results: List[Dict[str, Any]] = []

        # PATH-BASED QUERIES
        if query.path_patterns:
            path_results: List[Dict[str, Any]] = []
            for pattern in query.path_patterns:
                all_paths: List[List[Tuple[str, str, str]]] = []
                for nid, node_data in self.nodes.items():
                    if node_data['label'] == pattern.start_label:
                        self._find_paths(pattern, set(), nid, pattern.end_label, [], all_paths)
                        query._nodes_scanned += 1

                # Convert each path => result dict
                for path in all_paths:
                    # A path is a list of (node_id, edge_id, prev_node)
                    # path[0] = (start_node, '', '')
                    # subsequent items have (node_id, edge_id, prev_node)
                    if len(path) < 2:
                        # Means no edges => skip
                        continue
                    start_node_id = path[0][0]
                    end_node_id = path[-1][0]
                    path_res = {
                        'start_node': {
                            'id': start_node_id,
                            **self.nodes[start_node_id]
                        },
                        'end_node': {
                            'id': end_node_id,
                            **self.nodes[end_node_id]
                        },
                        'relationships': []
                    }
                    for step in path[1:]:
                        (n_id, e_id, p_node) = step
                        if e_id:
                            e_data = self.edges[e_id]
                            path_res['relationships'].append({
                                'id': e_id,
                                **e_data
                            })
                    path_results.append(path_res)
                    query._edges_traversed += (len(path) - 1)

            # Filter out any paths if query.relationship_filters fail
            if query.relationship_filters:
                final_paths = []
                for p_r in path_results:
                    rels = p_r.get('relationships', [])
                    keep = True
                    for rel in rels:
                        for f_func in query.relationship_filters:
                            if not f_func(rel):
                                keep = False
                                break
                        if not keep:
                            break
                    if keep and rels:
                        final_paths.append(p_r)
                results = final_paths
            else:
                # Also skip any path with 0 relationships
                results = [p for p in path_results if p['relationships']]

        # NODE-BASED QUERIES
        else:
            node_list: List[Dict[str, Any]] = []
            for nid, node_data in self.nodes.items():
                if query.matches_node(node_data):
                    node_list.append({'id': nid, **node_data})
                query._nodes_scanned += 1
            results = node_list

            # Aggregations => single record
            if query.aggregations and results and 'relationships' not in results[0]:
                agg_res = {}
                for agg in query.aggregations:
                    if agg.type == AggregationType.COUNT:
                        val = len(results)
                    else:
                        # numeric
                        values = []
                        for r in results:
                            props = r.get('properties', {})
                            if agg.field in props:
                                try:
                                    values.append(float(props[agg.field]))
                                except ValueError:
                                    pass
                        if values:
                            if agg.type == AggregationType.SUM:
                                val = sum(values)
                            elif agg.type == AggregationType.AVG:
                                val = sum(values) / len(values)
                            elif agg.type == AggregationType.MIN:
                                val = min(values)
                            elif agg.type == AggregationType.MAX:
                                val = max(values)
                        else:
                            val = None
                    alias = agg.alias or agg.field or agg.type.name.lower()
                    agg_res[alias] = val

                if agg_res:
                    return [agg_res]

        #
        # Sort node-based results by numeric property if possible
        #
        if query.sort_key:
            def sort_key_func(x):
                # If path-based => we skip numeric sorting
                if 'relationships' in x:
                    return 0
                val = x['properties'].get(query.sort_key, 0)
                try:
                    return float(val)
                except (TypeError, ValueError):
                    return val
            results.sort(key=sort_key_func, reverse=query.sort_reverse)

        #
        # Pagination
        #
        if query.skip is not None:
            results = results[query.skip:]
        if query.limit is not None:
            results = results[:query.limit]

        return results

    #
    # DFS HELPER
    #
    def _find_paths(
        self,
        pattern: PathPattern,
        visited: Set[str],
        current_node: str,
        target_label: str,
        path: List[Tuple[str, str, str]],
        paths: List[List[Tuple[str, str, str]]],
        depth: int = 0
    ) -> None:
        """
        DFS from current_node => any node labeled target_label,
        only along edges whose label is in pattern.relationships,
        up to pattern.max_depth, min_depth for final path acceptance.
        """
        if current_node in visited:
            return
        if depth > (pattern.max_depth or float('inf')):
            return

        visited.add(current_node)
        current_data = self.nodes[current_node]
        current_path = path or [(current_node, '', '')]

        # If reached target_label and depth >= min_depth => record path
        # Even if it's min_depth=1, we do not add path unless it has at least 1 edge
        if current_data['label'] == target_label and depth >= (pattern.min_depth or 0):
            # length of path must be at least 2 => means 1 edge
            if len(current_path) > 1:
                paths.append(current_path)

        # Explore forward edges from this node
        for (edge_id, next_node) in self._edge_index[current_node]:
            edge = self.edges[edge_id]
            if edge['label'] in pattern.relationships:
                new_path = current_path + [(next_node, edge_id, current_node)]
                self._find_paths(
                    pattern,
                    visited.copy(),
                    next_node,
                    target_label,
                    new_path,
                    paths,
                    depth + 1
                )

    #
    # BATCH IMPLEMENTATIONS
    #
    def _batch_create_nodes_impl(self, nodes: List[Dict[str, Any]]) -> List[str]:
        node_ids = []
        for n in nodes:
            node_id = str(uuid.uuid4())
            self.nodes[node_id] = {
                'label': n['label'],
                'properties': n['properties']
            }
            node_ids.append(node_id)
        return node_ids

    def _batch_create_edges_impl(self, edges: List[Dict[str, Any]]) -> List[str]:
        edge_ids = []
        for e in edges:
            from_id = str(e['from_id'])
            to_id = str(e['to_id'])
            if from_id not in self.nodes or to_id not in self.nodes:
                raise ValueError(f"Source or target node does not exist for edge: {e}")

            edge_id = str(uuid.uuid4())
            self.edges[edge_id] = {
                'from_id': from_id,
                'to_id': to_id,
                'label': e['label'],
                'properties': e.get('properties', {})
            }
            edge_ids.append(edge_id)

        self._update_edge_index()
        return edge_ids

================================================================================


================================================================================
FILE: graphrouter/monitoring.py
================================================================================
# graphrouter/monitoring.py

"""
Performance monitoring for GraphRouter.
"""
import time
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from statistics import mean, median, stdev
from datetime import datetime, timedelta

class OperationMetrics:
    """Holds metrics for a specific operation type."""
    def __init__(self):
        self.durations: List[float] = []
        self.timestamps: List[datetime] = []
        self.errors: int = 0
        self.last_error: Optional[str] = None

    def add_duration(self, duration: float):
        """Add a new duration measurement."""
        self.durations.append(duration)
        self.timestamps.append(datetime.now())

    def record_error(self, error_msg: str):
        """Record an operation error."""
        self.errors += 1
        self.last_error = error_msg

    def get_stats(self) -> Dict[str, float]:
        """Calculate statistics for this operation."""
        if not self.durations:
            return {
                'count': 0,
                'avg_duration': 0.0,
                'median_duration': 0.0,
                'min_duration': 0.0,
                'max_duration': 0.0,
                'std_dev': 0.0,
                'error_rate': 0.0
            }

        total_ops = len(self.durations)
        return {
            'count': total_ops,
            'avg_duration': mean(self.durations),
            'median_duration': median(self.durations),
            'min_duration': min(self.durations),
            'max_duration': max(self.durations),
            'std_dev': stdev(self.durations) if len(self.durations) > 1 else 0.0,
            'error_rate': self.errors / total_ops if total_ops > 0 else 0.0
        }

    def cleanup_old_metrics(self, cutoff: datetime):
        """Remove metrics older than the cutoff time."""
        if not self.timestamps:
            return

        valid_indices = [i for i, ts in enumerate(self.timestamps) if ts >= cutoff]
        self.durations = [self.durations[i] for i in valid_indices]
        self.timestamps = [self.timestamps[i] for i in valid_indices]

    def __len__(self):
        """Return the number of durations recorded."""
        return len(self.durations)

class PerformanceMonitor:
    def __init__(self, metrics_ttl: int = 3600):
        """Initialize the performance monitor.

        Args:
            metrics_ttl: Time to live for metrics in seconds (default: 1 hour)
        """
        self.metrics: Dict[str, OperationMetrics] = defaultdict(OperationMetrics)
        self.metrics_ttl = metrics_ttl

    def record_operation(self, operation: str, duration: float, error: Optional[str] = None):
        """Record an operation's execution metrics.

        Args:
            operation: Name of the operation
            duration: Execution time in seconds
            error: Error message if the operation failed
        """
        metrics = self.metrics[operation]
        metrics.add_duration(duration)
        if error:
            metrics.record_error(error)

    def get_average_times(self) -> Dict[str, float]:
        """Get average execution time for each operation type."""
        return {
            op: metrics.get_stats()['avg_duration']
            for op, metrics in self.metrics.items()
        }

    def get_detailed_metrics(self) -> Dict[str, Dict[str, float]]:
        """Get detailed metrics for all operations."""
        self._cleanup_old_metrics()
        return {
            op: metrics.get_stats()
            for op, metrics in self.metrics.items()
        }

    def get_operation_stats(self, operation: str) -> Dict[str, float]:
        """Get detailed statistics for a specific operation."""
        if operation not in self.metrics:
            return {}
        return self.metrics[operation].get_stats()

    def _cleanup_old_metrics(self):
        """Remove metrics older than TTL."""
        cutoff = datetime.now() - timedelta(seconds=self.metrics_ttl)
        for metrics in self.metrics.values():
            metrics.cleanup_old_metrics(cutoff)

    def reset(self):
        """Clear all metrics."""
        self.metrics.clear()

================================================================================


================================================================================
FILE: graphrouter/neo4j.py
================================================================================
"""
Neo4j graph database backend implementation.
"""
from typing import Dict, List, Any, Optional, Union, cast
from neo4j import GraphDatabase as Neo4jDriver, Session
from neo4j.exceptions import ServiceUnavailable, AuthError
from timeout_decorator import timeout
from .base import GraphDatabase
from .errors import ConnectionError, QueryError
from .query import Query

class Neo4jGraphDatabase(GraphDatabase):
    """Neo4j graph database implementation."""

    def __init__(self, pool_size: int = 5):
        super().__init__(pool_size=pool_size)
        self.driver: Optional[Neo4jDriver] = None
        self.uri: Optional[str] = None
        self.auth: Optional[tuple[str, str]] = None

    def connect(self, uri: str, username: str, password: str) -> bool:
        """Connect to Neo4j database.

        Args:
            uri: The Neo4j connection URI (e.g., 'bolt://localhost:7687')
            username: Neo4j username
            password: Neo4j password

        Returns:
            bool: True if connection successful

        Raises:
            ConnectionError: If connection fails
        """
        try:
            self.uri = uri
            self.auth = (username, password)
            self.driver = Neo4jDriver.driver(uri, auth=self.auth)

            # Test connection
            with self.driver.session() as session:
                session.run("RETURN 1")
            self.connected = True
            return True
        except AuthError as e:
            raise ConnectionError(f"Authentication failed: {str(e)}")
        except ServiceUnavailable as e:
            raise ConnectionError(f"Neo4j service unavailable: {str(e)}")
        except Exception as e:
            raise ConnectionError(f"Failed to connect to Neo4j: {str(e)}")

    def disconnect(self) -> bool:
        """Disconnect from the database."""
        if self.driver:
            self.driver.close()
            self.driver = None
        self.connected = False
        return True

    @timeout(30)  # 30 second timeout for operations
    def _execute_with_retry(self, operation, *args, **kwargs):
        """Execute an operation with retry logic."""
        if not self.connected or not self.driver:
            raise ConnectionError("Database not connected")

        try:
            with self.driver.session() as session:
                return operation(session, *args, **kwargs)
        except ServiceUnavailable:
            # Try to reconnect once
            if self.uri and self.auth:
                self.connect(self.uri, self.auth[0], self.auth[1])
                with self.driver.session() as session:
                    return operation(session, *args, **kwargs)
            raise
        except Exception as e:
            raise QueryError(f"Operation failed: {str(e)}")

    def create_node(self, label: str, properties: Dict[str, Any]) -> str:
        """Create a new node with the given label and properties."""
        return self._create_node_impl(label, properties)

    def _create_node_impl(self, label: str, properties: Dict[str, Any]) -> str:
        """Implementation of create_node operation."""
        def create_node_op(session: Session, label: str, properties: Dict[str, Any]) -> str:
            query = (
                f"CREATE (n:{label} $props) "
                "RETURN id(n) as node_id"
            )
            result = session.run(query, props=properties)
            record = result.single()
            return str(record["node_id"])

        return self._execute_with_retry(create_node_op, label, properties)

    def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve a node by its ID."""
        return self._get_node_impl(node_id)

    def _get_node_impl(self, node_id: str) -> Optional[Dict[str, Any]]:
        """Implementation of get_node operation."""
        def get_node_op(session: Session, node_id: str) -> Optional[Dict[str, Any]]:
            query = (
                "MATCH (n) "
                "WHERE id(n) = $node_id "
                "RETURN labels(n) as label, properties(n) as properties"
            )
            result = session.run(query, node_id=int(node_id))
            record = result.single()
            if record:
                return {
                    'label': record["label"][0],  # Neo4j can have multiple labels, we take the first
                    'properties': record["properties"]
                }
            return None

        return self._execute_with_retry(get_node_op, node_id)

    def update_node(self, node_id: str, properties: Dict[str, Any]) -> bool:
        """Update a node's properties."""
        return self._update_node_impl(node_id, properties)

    def _update_node_impl(self, node_id: str, properties: Dict[str, Any]) -> bool:
        """Implementation of update_node operation."""
        def update_node_op(session: Session, node_id: str, properties: Dict[str, Any]) -> bool:
            query = (
                "MATCH (n) "
                "WHERE id(n) = $node_id "
                "SET n += $props "
                "RETURN n"
            )
            result = session.run(query, node_id=int(node_id), props=properties)
            return bool(result.single())

        return self._execute_with_retry(update_node_op, node_id, properties)

    def delete_node(self, node_id: str) -> bool:
        """Delete a node by its ID."""
        return self._delete_node_impl(node_id)

    def _delete_node_impl(self, node_id: str) -> bool:
        """Implementation of delete_node operation."""
        def delete_node_op(session: Session, node_id: str) -> bool:
            query = (
                "MATCH (n) "
                "WHERE id(n) = $node_id "
                "DETACH DELETE n"
            )
            session.run(query, node_id=int(node_id))
            return True

        return self._execute_with_retry(delete_node_op, node_id)

    def create_edge(self, from_id: str, to_id: str, label: str, properties: Optional[Dict[str, Any]] = None) -> str:
        """Create an edge between two nodes."""
        return self._create_edge_impl(from_id, to_id, label, properties)

    def _create_edge_impl(self, from_id: str, to_id: str, label: str, properties: Optional[Dict[str, Any]] = None) -> str:
        """Implementation of create_edge operation."""
        def create_edge_op(session: Session, from_id: str, to_id: str, label: str, properties: Dict[str, Any]) -> str:
            query = (
                "MATCH (a), (b) "
                "WHERE id(a) = $from_id AND id(b) = $to_id "
                f"CREATE (a)-[r:{label} $props]->(b) "
                "RETURN id(r) as edge_id"
            )
            result = session.run(
                query,
                from_id=int(from_id),
                to_id=int(to_id),
                props=properties or {}
            )
            record = result.single()
            return str(record["edge_id"])

        return self._execute_with_retry(create_edge_op, from_id, to_id, label, properties or {})

    def get_edge(self, edge_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve an edge by its ID."""
        return self._get_edge_impl(edge_id)

    def _get_edge_impl(self, edge_id: str) -> Optional[Dict[str, Any]]:
        """Implementation of get_edge operation."""
        def get_edge_op(session: Session, edge_id: str) -> Optional[Dict[str, Any]]:
            query = (
                "MATCH ()-[r]->() "
                "WHERE id(r) = $edge_id "
                "RETURN type(r) as label, properties(r) as properties, "
                "id(startNode(r)) as from_id, id(endNode(r)) as to_id"
            )
            result = session.run(query, edge_id=int(edge_id))
            record = result.single()
            if record:
                return {
                    'label': record["label"],
                    'properties': record["properties"],
                    'from_id': str(record["from_id"]),
                    'to_id': str(record["to_id"])
                }
            return None

        return self._execute_with_retry(get_edge_op, edge_id)

    def update_edge(self, edge_id: str, properties: Dict[str, Any]) -> bool:
        """Update an edge's properties."""
        return self._update_edge_impl(edge_id, properties)

    def _update_edge_impl(self, edge_id: str, properties: Dict[str, Any]) -> bool:
        """Implementation of update_edge operation."""
        def update_edge_op(session: Session, edge_id: str, properties: Dict[str, Any]) -> bool:
            query = (
                "MATCH ()-[r]->() "
                "WHERE id(r) = $edge_id "
                "SET r += $props "
                "RETURN r"
            )
            result = session.run(query, edge_id=int(edge_id), props=properties)
            return bool(result.single())

        return self._execute_with_retry(update_edge_op, edge_id, properties)

    def delete_edge(self, edge_id: str) -> bool:
        """Delete an edge by its ID."""
        return self._delete_edge_impl(edge_id)

    def _delete_edge_impl(self, edge_id: str) -> bool:
        """Implementation of delete_edge operation."""
        def delete_edge_op(session: Session, edge_id: str) -> bool:
            query = (
                "MATCH ()-[r]->() "
                "WHERE id(r) = $edge_id "
                "DELETE r"
            )
            session.run(query, edge_id=int(edge_id))
            return True

        return self._execute_with_retry(delete_edge_op, edge_id)

    def batch_create_nodes(self, nodes: List[Dict[str, Any]]) -> List[str]:
        """Create multiple nodes in a single operation."""
        return self._batch_create_nodes_impl(nodes)

    def _batch_create_nodes_impl(self, nodes: List[Dict[str, Any]]) -> List[str]:
        """Implementation of batch_create_nodes operation."""
        def batch_create_nodes_op(session: Session, nodes: List[Dict[str, Any]]) -> List[str]:
            queries = []
            params = {}

            for i, node in enumerate(nodes):
                if 'label' not in node or 'properties' not in node:
                    raise ValueError("Invalid node format")

                if not self.validate_node(node['label'], node['properties']):
                    raise ValueError(f"Node validation failed for node: {node}")

                param_name = f"props_{i}"
                queries.append(f"CREATE (n:{node['label']} ${param_name}) RETURN id(n) as node_id")
                params[param_name] = node['properties']

            query = " UNION ALL ".join(queries)
            result = session.run(query, params)
            return [str(record["node_id"]) for record in result]

        return self._execute_with_retry(batch_create_nodes_op, nodes)

    def batch_create_edges(self, edges: List[Dict[str, Any]]) -> List[str]:
        """Create multiple edges in a single operation."""
        return self._batch_create_edges_impl(edges)

    def _batch_create_edges_impl(self, edges: List[Dict[str, Any]]) -> List[str]:
        """Implementation of batch_create_edges operation."""
        def batch_create_edges_op(session: Session, edges: List[Dict[str, Any]]) -> List[str]:
            queries = []
            params = {}

            for i, edge in enumerate(edges):
                if not all(k in edge for k in ['from_id', 'to_id', 'label']):
                    raise ValueError("Invalid edge format")

                properties = edge.get('properties', {})
                if not self.validate_edge(edge['label'], properties):
                    raise ValueError(f"Edge validation failed for edge: {edge}")

                from_param = f"from_{i}"
                to_param = f"to_{i}"
                props_param = f"props_{i}"

                queries.append(
                    f"MATCH (a), (b) "
                    f"WHERE id(a) = ${from_param} AND id(b) = ${to_param} "
                    f"CREATE (a)-[r:{edge['label']} ${props_param}]->(b) "
                    "RETURN id(r) as edge_id"
                )

                params[from_param] = int(edge['from_id'])
                params[to_param] = int(edge['to_id'])
                params[props_param] = properties

            query = " UNION ALL ".join(queries)
            result = session.run(query, params)
            return [str(record["edge_id"]) for record in result]

        return self._execute_with_retry(batch_create_edges_op, edges)

    def query(self, query: Query) -> List[Dict[str, Any]]:
        """Execute a query and return results."""
        return self._query_impl(query)

    def _query_impl(self, query: Query) -> List[Dict[str, Any]]:
        """Implementation of query operation."""
        def query_op(session: Session, query: Query) -> List[Dict[str, Any]]:
            cypher = self._build_cypher_query(query)
            result = session.run(cypher)

            results = []
            for record in result:
                node = record["n"]
                results.append({
                    'id': str(node.id),
                    'label': list(node.labels)[0],  # Get the first label
                    'properties': dict(node)
                })
            return results

        return self._execute_with_retry(query_op, query)

    def _build_cypher_query(self, query: Query) -> str:
        """Convert Query object to Cypher query string."""
        parts = ["MATCH (n)"]
        where_clauses = []

        for filter_func in query.filters:
            if hasattr(filter_func, 'filter_type'):
                filter_type = getattr(filter_func, 'filter_type')
                if filter_type == 'label_equals':
                    where_clauses.append(f"n:{getattr(filter_func, 'label')}")
                elif filter_type == 'property_equals':
                    where_clauses.append(
                        f"n.{getattr(filter_func, 'property_name')} = {repr(getattr(filter_func, 'value'))}"
                    )
                elif filter_type == 'property_contains':
                    where_clauses.append(
                        f"n.{getattr(filter_func, 'property_name')} CONTAINS {repr(getattr(filter_func, 'value'))}"
                    )

        if where_clauses:
            parts.append("WHERE " + " AND ".join(where_clauses))

        if query.sort_key:
            direction = "DESC" if query.sort_reverse else "ASC"
            parts.append(f"ORDER BY n.{query.sort_key} {direction}")

        if query.limit:
            parts.append(f"LIMIT {query.limit}")

        parts.append("RETURN n")
        return " ".join(parts)
================================================================================


================================================================================
FILE: graphrouter/ontology.py
================================================================================
"""
Ontology management for graph databases.
"""
from typing import Dict, Any, List, Optional
from .errors import InvalidPropertyError, InvalidNodeTypeError

class Ontology:
    """Manages the ontology (schema) for the graph database."""

    def __init__(self):
        self.node_types = {}
        self.edge_types = {}

    def add_node_type(self, label: str, properties: Dict[str, str], required: List[str] = None):
        """Add a node type to the ontology."""
        self.node_types[label] = {
            'properties': properties,
            'required': required or []
        }

    def add_edge_type(self, label: str, properties: Dict[str, str], required: List[str] = None):
        """Add an edge type to the ontology."""
        self.edge_types[label] = {
            'properties': properties,
            'required': required or []
        }

    def validate_node(self, label: str, properties: Dict[str, Any]) -> bool:
        """Validate a node against the ontology. Raises on invalid data."""
        if label not in self.node_types:
            available_types = list(self.node_types.keys())
            raise InvalidNodeTypeError(
                f"Invalid node type '{label}'. Available types: {', '.join(available_types)}",
                {"available_types": available_types}
            )

        node_type = self.node_types[label]

        # Check required properties
        missing_required = []
        for req_prop in node_type['required']:
            if req_prop not in properties:
                missing_required.append(req_prop)

        if missing_required:
            raise InvalidPropertyError(
                f"Missing required properties for node type '{label}': {', '.join(missing_required)}",
                {
                    "required_properties": node_type['required'],
                    "missing_properties": missing_required,
                    "available_properties": list(node_type['properties'].keys())
                }
            )

        # Validate property types
        invalid_props = []
        for prop_name, prop_value in properties.items():
            if prop_name in node_type['properties']:
                expected_type = node_type['properties'][prop_name]
                if not isinstance(prop_value, eval(expected_type)):
                    invalid_props.append((prop_name, expected_type))
            else:
                # Unknown property
                raise InvalidPropertyError(
                    f"Unknown property '{prop_name}' for node type '{label}'. "
                    f"Available properties: {', '.join(node_type['properties'].keys())}",
                    {"available_properties": list(node_type['properties'].keys())}
                )

        if invalid_props:
            details = [f"'{name}' (expected {typ})" for name, typ in invalid_props]
            raise InvalidPropertyError(
                f"Invalid property types for node type '{label}': {', '.join(details)}",
                {
                    "invalid_properties": dict(invalid_props),
                    "all_properties": node_type['properties']
                }
            )

        return True

    def validate_edge(self, label: str, properties: Dict[str, Any]) -> bool:
        """Validate an edge against the ontology. Raises on invalid data."""
        if label not in self.edge_types:
            available = list(self.edge_types.keys())
            raise InvalidNodeTypeError(
                f"Invalid edge type '{label}'. Available edge types: {', '.join(available)}",
                {"available_types": available}
            )

        edge_type = self.edge_types[label]

        # Check required properties
        missing_required = []
        for req_prop in edge_type['required']:
            if req_prop not in properties:
                missing_required.append(req_prop)

        if missing_required:
            raise InvalidPropertyError(
                f"Missing required properties for edge type '{label}': {', '.join(missing_required)}",
                {
                    "required_properties": edge_type['required'],
                    "missing_properties": missing_required,
                    "available_properties": list(edge_type['properties'].keys())
                }
            )

        # Validate property types
        invalid_props = []
        for prop_name, prop_value in properties.items():
            if prop_name in edge_type['properties']:
                expected_type = edge_type['properties'][prop_name]
                if not isinstance(prop_value, eval(expected_type)):
                    invalid_props.append((prop_name, expected_type))
            else:
                # Unknown property
                raise InvalidPropertyError(
                    f"Unknown property '{prop_name}' for edge type '{label}'. "
                    f"Available properties: {', '.join(edge_type['properties'].keys())}",
                    {"available_properties": list(edge_type['properties'].keys())}
                )

        if invalid_props:
            details = [f"'{name}' (expected {typ})" for name, typ in invalid_props]
            raise InvalidPropertyError(
                f"Invalid property types for edge type '{label}': {', '.join(details)}",
                {
                    "invalid_properties": dict(invalid_props),
                    "all_properties": edge_type['properties']
                }
            )

        return True

    def to_dict(self) -> Dict:
        """Convert the ontology to a dictionary."""
        return {
            'node_types': self.node_types,
            'edge_types': self.edge_types
        }

    @classmethod
    def from_dict(cls, data: Dict) -> 'Ontology':
        """Create an ontology from a dictionary."""
        ontology = cls()
        ontology.node_types = data.get('node_types', {})
        ontology.edge_types = data.get('edge_types', {})
        return ontology

    def map_node_properties(self, label: str, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Map properties to match ontology schema for a node type."""
        if label not in self.node_types:
            return properties  # or raise, but the top-level validate handles it

        schema = self.node_types[label]['properties']
        mapped = {}
        for prop, value in properties.items():
            if prop in schema:
                try:
                    mapped[prop] = eval(schema[prop])(value)
                except (ValueError, TypeError):
                    continue
        # Add defaults for missing required
        for req in self.node_types[label]['required']:
            if req not in mapped and req in schema:
                mapped[req] = eval(schema[req])()
        return mapped

    def map_edge_properties(self, label: str, properties: Dict[str, Any]) -> Dict[str, Any]:
        """Map properties to match ontology schema for an edge type."""
        if label not in self.edge_types:
            return properties

        schema = self.edge_types[label]['properties']
        mapped = {}
        for prop, value in properties.items():
            if prop in schema:
                try:
                    mapped[prop] = eval(schema[prop])(value)
                except (ValueError, TypeError):
                    continue
        # Add defaults for missing required
        for req in self.edge_types[label]['required']:
            if req not in mapped and req in schema:
                mapped[req] = eval(schema[req])()
        return mapped

================================================================================


================================================================================
FILE: graphrouter/query.py
================================================================================
"""
Query builder and executor for graph databases.
"""
from typing import Dict, Any, List, Optional, Callable, Union, TypeVar, Generic
from enum import Enum

T = TypeVar('T')

class AggregationType(Enum):
    """Supported aggregation types."""
    COUNT = "count"
    SUM = "sum"
    AVG = "avg"
    MIN = "min"
    MAX = "max"

class PathPattern:
    """Represents a path pattern for graph traversal."""
    def __init__(self, start_label: str, end_label: str, relationships: List[str],
                min_depth: Optional[int] = None, max_depth: Optional[int] = None):
        self.start_label = start_label
        self.end_label = end_label
        self.relationships = relationships
        self.min_depth = min_depth
        self.max_depth = max_depth

class Aggregation:
    """Represents an aggregation operation."""
    def __init__(self, type: AggregationType, field: Optional[str] = None, alias: Optional[str] = None):
        self.type = type
        self.field = field
        self.alias = alias or f"{type.value}_{field if field else 'result'}"

class Query:
    """Query builder for graph databases."""

    def __init__(self):
        self.filters: List[Callable[[Dict[str, Any]], bool]] = []
        self.relationship_filters: List[Callable[[Dict[str, Any]], bool]] = []
        self.sort_key: Optional[str] = None
        self.sort_reverse: bool = False
        self.limit: Optional[int] = None
        self.skip: Optional[int] = None
        self.page: Optional[int] = None
        self.page_size: Optional[int] = None
        self.path_patterns: List[PathPattern] = []
        self.aggregations: List[Aggregation] = []
        self._nodes_scanned = 0
        self._edges_traversed = 0
        self._execution_time = 0.0
        self._memory_used = 0.0

    def filter(self, condition: Callable[[Dict[str, Any]], bool]) -> 'Query':
        """Add a filter condition to the query.

        Args:
            condition: A callable that takes a node dict and returns bool

        Returns:
            self for chaining
        """
        self.filters.append(condition)
        return self

    def filter_relationship(self, condition: Callable[[Dict[str, Any]], bool]) -> 'Query':
        """Add a filter condition for relationships.

        Args:
            condition: A callable that takes a relationship dict and returns bool

        Returns:
            self for chaining
        """
        self.relationship_filters.append(condition)
        return self

    def sort(self, key: str, reverse: bool = False) -> 'Query':
        """Sort results by a property key.

        Args:
            key: Property key to sort by
            reverse: If True, sort in descending order

        Returns:
            self for chaining
        """
        self.sort_key = key
        self.sort_reverse = reverse
        return self

    def limit_results(self, limit: int) -> 'Query':
        """Limit the number of results.

        Args:
            limit: Maximum number of results to return

        Returns:
            self for chaining

        Raises:
            ValueError: If limit is negative
        """
        if limit < 0:
            raise ValueError("Limit must be non-negative")
        self.limit = limit
        return self

    def matches_node(self, node: Dict[str, Any]) -> bool:
        """Check if a node matches all filter conditions.

        Args:
            node: Node dict to check

        Returns:
            True if node matches all filters
        """
        return all(f(node) for f in self.filters)

    def matches_relationship(self, relationship: Dict[str, Any]) -> bool:
        """Check if a relationship matches all filter conditions.

        Args:
            relationship: Relationship dict to check

        Returns:
            True if relationship matches all filters
        """
        return all(f(relationship) for f in self.relationship_filters)

    @staticmethod
    def property_equals(property_name: str, value: Any) -> Callable[[Dict[str, Any]], bool]:
        """Create a filter for property equality.

        Args:
            property_name: Name of the property to check
            value: Value to compare against

        Returns:
            Filter function
        """
        filter_type = 'property_equals'
        def filter_func(node: Dict[str, Any]) -> bool:
            return node.get('properties', {}).get(property_name) == value
        filter_func.filter_type = filter_type
        filter_func.property_name = property_name
        filter_func.value = value
        return filter_func

    @staticmethod
    def property_contains(property_name: str, value: Any) -> Callable[[Dict[str, Any]], bool]:
        """Create a filter for property containment.

        Args:
            property_name: Name of the property to check
            value: Value to check for containment

        Returns:
            Filter function
        """
        filter_type = 'property_contains'
        def filter_func(node: Dict[str, Any]) -> bool:
            node_value = node.get('properties', {}).get(property_name)
            return value in node_value if node_value is not None else False
        filter_func.filter_type = filter_type
        filter_func.property_name = property_name
        filter_func.value = value
        return filter_func

    @staticmethod
    def label_equals(label: str) -> Callable[[Dict[str, Any]], bool]:
        """Create a filter for label equality.

        Args:
            label: Label to match

        Returns:
            Filter function
        """
        filter_type = 'label_equals'
        def filter_func(node: Dict[str, Any]) -> bool:
            return node.get('label') == label
        filter_func.filter_type = filter_type
        filter_func.label = label
        return filter_func

    def add_path_pattern(self, pattern: PathPattern) -> 'Query':
        """Add a path pattern to search for.

        Args:
            pattern: Path pattern to add

        Returns:
            self for chaining
        """
        self.path_patterns.append(pattern)
        return self

    def find_path(self, start_label: str, end_label: str, relationships: List[str],
                min_depth: Optional[int] = None, max_depth: Optional[int] = None) -> 'Query':
        """Find paths between nodes matching given patterns.

        Args:
            start_label: Label of start nodes
            end_label: Label of end nodes
            relationships: Relationship types to traverse
            min_depth: Minimum path length
            max_depth: Maximum path length

        Returns:
            self for chaining
        """
        pattern = PathPattern(start_label, end_label, relationships, min_depth, max_depth)
        return self.add_path_pattern(pattern)

    def aggregate(self, type: AggregationType, field: Optional[str] = None,
                alias: Optional[str] = None) -> 'Query':
        """Add an aggregation operation.

        Args:
            type: Type of aggregation
            field: Field to aggregate
            alias: Result alias

        Returns:
            self for chaining
        """
        self.aggregations.append(Aggregation(type, field, alias))
        return self

    def paginate(self, page: int, page_size: int) -> 'Query':
        """Add pagination to query results.

        Args:
            page: Page number (1-based)
            page_size: Results per page

        Returns:
            self for chaining

        Raises:
            ValueError: If page or page_size is less than 1
        """
        if page < 1:
            raise ValueError("Page number must be positive")
        if page_size < 1:
            raise ValueError("Page size must be positive")
        self.page = page
        self.page_size = page_size
        self.limit = page_size
        self.skip = (page - 1) * page_size
        return self

    def collect_stats(self) -> Dict[str, Any]:
        """Collect query execution statistics.

        Returns:
            Dict with execution statistics
        """
        return {
            'nodes_scanned': self._nodes_scanned,
            'edges_traversed': self._edges_traversed,
            'execution_time': self._execution_time,
            'memory_used': self._memory_used
        }
================================================================================


================================================================================
FILE: graphrouter/query_builder.py
================================================================================

"""
Fluent query builder interface for GraphRouter.
"""
from typing import Any, Dict, List, Optional
from .query import Query

class QueryBuilder:
    def __init__(self):
        self.query = Query()
    
    def with_label(self, label: str) -> 'QueryBuilder':
        self.query.filter(Query.label_equals(label))
        return self
        
    def where(self, property_name: str, value: Any) -> 'QueryBuilder':
        self.query.filter(Query.property_equals(property_name, value))
        return self
        
    def order_by(self, property_name: str, descending: bool = False) -> 'QueryBuilder':
        self.query.sort(property_name, reverse=descending)
        return self
        
    def limit(self, count: int) -> 'QueryBuilder':
        self.query.limit_results(count)
        return self
        
    def build(self) -> Query:
        return self.query

================================================================================


================================================================================
FILE: graphrouter/transaction.py
================================================================================

"""
Transaction management for GraphRouter.
"""
from typing import Any, Callable
from contextlib import contextmanager

class Transaction:
    def __init__(self, db_instance):
        self.db = db_instance
        self.operations = []
        
    def add_operation(self, operation: Callable, *args, **kwargs):
        self.operations.append((operation, args, kwargs))
        
    def commit(self):
        try:
            for op, args, kwargs in self.operations:
                op(*args, **kwargs)
            return True
        except Exception:
            self.rollback()
            return False
            
    def rollback(self):
        self.operations = []

@contextmanager
def transaction_scope(db_instance):
    transaction = Transaction(db_instance)
    try:
        yield transaction
        transaction.commit()
    except Exception:
        transaction.rollback()
        raise

================================================================================


================================================================================
FILE: ingestion_engine/README.md
================================================================================
# Ingestion Engine

Welcome to the **Ingestion Engine** README! This engine is designed to integrate seamlessly with [GraphRouter](../graphrouter) for graph database operations and [Composio](https://github.com/composiohq) for external integrations and authentication. The Ingestion Engine handles tasks like file uploads (with CSV auto-parsing), data syncing (e.g., using webhooks or APIs), and optional auto-extraction of structured logs, all while linking data to a standardized **default/core ontology**.

[Rest of the first attachment content, with relative paths updated]

auto_extract_structured_data (bool): Whether to automatically extract structured data
schedule_interval (Optional[int]): How often (in seconds) to run periodic sync jobs
================================================================================


================================================================================
FILE: ingestion_engine/ingestion_engine.py
================================================================================
"""
Ingestion Engine for Automated Graph Updates

This engine handles:

1. File Upload (with CSV auto-parsing)
2. Authentication & Download (via Composio)
3. Regular Sync / Historical Data Collection
4. Search & Dedupe
5. Webhook Handling (auth-enabled)
6. Automatic Structured Extraction of Logs/Data
7. Linking to a default/core ontology for consistent node/relationship types

Dependencies:
- graphrouter (your unified interface to multiple graph DBs)
- composio (for authentication, hooking into external tool actions)
- python libraries for CSV parsing, scheduling, etc.
"""

import os
import csv
import time
import uuid
import json
import logging
from typing import Any, Dict, List, Optional, Union
from graphrouter.core_ontology import create_core_ontology, extend_ontology

# External: GraphRouter (example import; adapt to your actual usage)
from graphrouter import LocalGraphDatabase, Query  # or your chosen backend
# or: from graphrouter import Neo4jGraphDatabase, ...
# or: from graphrouter import <some-other-backend>

# External: Composio
# e.g., from composio import ComposioToolSet, Action
try:
    from composio import ComposioToolSet, Action
except ImportError:
    ComposioToolSet = None
    Action = None
    # Fallback or dummy placeholders if composio isn't installed

###############################################################################
# Default / Core Ontology
###############################################################################

CORE_ONTOLOGY = {
    "DataSource": {
        "description": "Represents a distinct data source (webhook, Airtable, Gmail, etc.)."
    },
    "File": {
        "description": "Represents an uploaded file, e.g., CSV, PDF, etc."
    },
    "Row": {
        "description": "Represents a single row from a parsed CSV or similar tabular structure."
    },
    "Log": {
        "description": "Represents a log or log entry associated with an ingestion event."
    },
    "SearchResult": {
        "description": "Represents a single search result (deduplicated if needed)."
    },
    "Webhook": {
        "description": "Represents an inbound or outbound webhook endpoint or event."
    },
    # Add additional core types as your system requires
}

###############################################################################
# Ingestion Engine Class
###############################################################################


class IngestionEngine:
    """
    A flexible engine for ingesting data into a graph via GraphRouter.
    Supports:
      - File upload (with CSV auto-parse).
      - Auth & download using Composio-based tools (tracking history, pagination).
      - Data sync (scheduled or on-demand).
      - Search and deduping search results in the graph.
      - Webhooks with auth flows.
      - Automatic structured extraction from logs or incoming data.

    Parameters:
        router_config (dict): Configuration for the GraphRouter backend.
        composio_config (dict): Configuration for setting up ComposioToolSet (API key, entity_id, etc.).
        auto_extract_structured_data (bool): Whether to automatically extract
          structured data from logs or direct ingest data based on user-defined rules.
        extraction_rules (dict): Rules for auto-extraction (e.g., columns to include/exclude).
          Example:
            {
              "include_columns": ["id", "timestamp", "message"],
              "exclude_columns": ["debug_info"]
            }
        default_ontology (dict): Optional override or extension of the CORE_ONTOLOGY.
        deduplicate_search_results (bool): If True, search results will be deduplicated
          before storing in the graph.
        schedule_interval (Optional[int]): If set, indicates how often (in seconds)
          to run a periodic sync job. Use 0 or None to disable.
    """
    def __init__(
        self,
        router_config: Dict[str, Any],
        composio_config: Optional[Dict[str, Any]] = None,
        auto_extract_structured_data: bool = False,
        extraction_rules: Optional[Dict[str, List[str]]] = None,
        default_ontology: Optional[Dict[str, Any]] = None,
        deduplicate_search_results: bool = True,
        schedule_interval: Optional[int] = None,
        llm_integration: Optional[Any] = None
    ):
        from llm_engine.node_processor import NodeProcessor
        from llm_engine.enrichment import NodeEnrichmentManager
        # Setup logging
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(logging.DEBUG)

        # GraphRouter initialization
        self.logger.info("Initializing GraphRouter backend.")
        # For example, a local JSON-based DB:
        self.db = LocalGraphDatabase()
        self.db.connect(**router_config)

        # If you prefer Neo4j, use:
        # self.db = Neo4jGraphDatabase()
        # self.db.connect(uri="bolt://localhost:7687", username="neo4j", password="password")

        # Composio initialization (optional)
        self.composio_toolset = None
        if composio_config and ComposioToolSet:
            self.logger.info("Initializing ComposioToolSet.")
            self.composio_toolset = ComposioToolSet(**composio_config)

        # Auto extraction settings
        self.auto_extract_structured_data = auto_extract_structured_data
        self.extraction_rules = extraction_rules or {}
        self.deduplicate_search_results = deduplicate_search_results

        # Initialize and set ontology
        core_ontology = create_core_ontology()
        if default_ontology:
            self.ontology = extend_ontology(core_ontology, default_ontology)
        else:
            self.ontology = core_ontology
            
        # Set ontology on database
        self.db.set_ontology(self.ontology)
        
        # Setup enrichment pipeline
        self.llm_integration = llm_integration
        if llm_integration:
            self.node_processor = NodeProcessor(llm_integration)
            self.enrichment_manager = NodeEnrichmentManager(self.node_processor)

        # Setup scheduling
        self.schedule_interval = schedule_interval

    ###########################################################################
    # Internal Helpers
    ###########################################################################
    def _create_node(self, label: str, properties: Dict[str, Any]) -> str:
        """Create a node in the graph with the given label and properties."""
        node_id = self.db.create_node(label, properties)
        return node_id

    def _create_or_find_data_source(self, source_name: str) -> str:
        """
        Create or find an existing DataSource node by name.
        Returns the node_id.
        """
        query = Query().filter(Query.label_equals("DataSource"))
        query.filter(Query.property_equals("name", source_name))
        results = self.db.query(query)
        if results:
            self.logger.debug(f"DataSource '{source_name}' found.")
            return results[0]['id']  # assuming the first result's ID
        else:
            self.logger.debug(f"Creating DataSource '{source_name}'.")
            return self._create_node("DataSource", {"name": source_name})

    def _auto_parse_csv(self, file_path: str, file_node_id: str) -> None:
        """
        Parse CSV file line by line; create a 'Row' node for each row.
        Link each row node to the parent File node with relationship 'HAS_ROW'.
        """
        with open(file_path, mode='r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row_data in reader:
                row_node_id = self._create_node("Row", dict(row_data))
                # create relationship: file_node_id -> row_node_id
                self.db.create_edge(
                    file_node_id,
                    row_node_id,
                    "HAS_ROW",
                    {}
                )

    def _extract_structured_data_if_applicable(self, log_data: Dict[str, Any]) -> None:
        """
        Optionally parse structured data from logs/incoming data based on
        extraction_rules. This could be used after a sync, a webhook event, etc.
        """
        if not self.auto_extract_structured_data:
            return

        # Example logic: if "include_columns" in rules, create/attach nodes
        include_cols = self.extraction_rules.get("include_columns", [])
        exclude_cols = self.extraction_rules.get("exclude_columns", [])
        filtered_data = {}

        for key, val in log_data.items():
            if (include_cols and key not in include_cols):
                continue
            if (exclude_cols and key in exclude_cols):
                continue
            filtered_data[key] = val

        # Create a "Log" node or some other type
        log_node_id = self._create_node("Log", filtered_data)
        self.logger.info(f"Auto-extracted log data into node {log_node_id}.")

    def _store_search_results(self, query_str: str, results: List[Any]) -> None:
        """
        Store search results in the graph, optionally deduplicating.
        The node label is 'SearchResult'.
        """
        for result_item in results:
            # Check if dedupe is requested
            if self.deduplicate_search_results:
                query = Query().filter(Query.label_equals("SearchResult"))
                query.filter(Query.property_equals("content", result_item))
                existing = self.db.query(query)
                if existing:
                    self.logger.debug(f"Skipping duplicate: {result_item}")
                    continue

            # Create a new 'SearchResult' node
            sr_node_id = self._create_node("SearchResult", {
                "content": result_item,
                "query_string": query_str,
            })
            self.logger.debug(f"Created SearchResult node: {sr_node_id}")

    ###########################################################################
    # Public APIs
    ###########################################################################
    def upload_file(
        self,
        file_path: str,
        data_source_name: str,
        parse_csv: bool = True
    ) -> str:
        """
        Upload a local file to the graph. Creates a 'File' node with relevant metadata.
        If CSV, optionally parse and create 'Row' nodes. The file is associated
        with a 'DataSource' node that references the origin or context.

        Returns:
          file_node_id (str): The graph ID of the newly created file node.
        """
        data_source_id = self._create_or_find_data_source(data_source_name)

        file_name = os.path.basename(file_path)
        file_node_id = self._create_node("File", {
            "file_name": file_name,
            "path": file_path,
            "uploaded_time": time.time()
        })

        # Link the file node to DataSource
        self.db.create_edge(data_source_id, file_node_id, "HAS_FILE", {})

        # Auto-parse CSV if requested
        if parse_csv and file_name.lower().endswith(".csv"):
            self.logger.info(f"Parsing CSV: {file_name}")
            self._auto_parse_csv(file_path, file_node_id)

        self.logger.info(f"File '{file_name}' uploaded. Node ID: {file_node_id}")
        return file_node_id

    def download_data(self, action: Union[str, None] = None, params: dict = None) -> Dict[str, Any]:
        """
        Download data using a Composio action. If no specific action is provided,
        developers can rely on the text-based approach (the `text` parameter).
        Example:
            - Action.GITHUB_GET_CONTENTS_OF_A_REPOSITORY
            - Action.GDRIVE_DOWNLOAD_FILE
        Keeps a history log node to track downloads.

        Returns:
          A dictionary with the downloaded data or metadata from the action.
        """
        if not self.composio_toolset:
            raise RuntimeError("ComposioToolSet not configured. Cannot download data.")

        # If the developer wants a natural language approach, they can do:
        #    params["text"] = "Download all data from my Gist"
        # Otherwise, the direct action-based approach is used.
        result = self.composio_toolset.execute_action(
            action=action,
            params=params or {}
        )
        # Store a "Log" node for the download action
        log_node_id = self._create_node("Log", {
            "timestamp": time.time(),
            "action": str(action),
            "params": json.dumps(params or {}),
            "result": json.dumps(result),
            "type": "download"
        })
        self.logger.info(f"Download log created: {log_node_id}")

        # Optionally auto-extract structured data from the result
        self._extract_structured_data_if_applicable(result)

        return result

    def sync_data(
        self,
        data_source_name: str,
        action: Optional[str] = None,
        params: Optional[Dict[str, Any]] = None
    ):
        """
        Sync data regularly from a particular source by calling a Composio action
        that fetches new data (potentially handling pagination or historical data).

        The results are stored as log nodes or specialized nodes depending on
        auto-extraction settings.
        """
        data_source_id = self._create_or_find_data_source(data_source_name)
        sync_result = self.download_data(action=action, params=params)
        # Link the resulting Log node to the DataSource
        # This depends on your usage pattern. For instance, if `_create_node("Log",...)`
        # is always called inside download_data, we can do a second query for that log:
        # or create a new node specifically for "SyncLog".
        sync_node_id = self._create_node("Log", {
            "type": "sync",
            "data_source": data_source_name,
            "details": json.dumps(sync_result),
            "timestamp": time.time()
        })
        self.db.create_edge(data_source_id, sync_node_id, "HAS_SYNC", {})
        self.logger.info(f"Sync data from '{data_source_name}' completed. Log: {sync_node_id}")

    def search_and_store_results(self, query_str: str) -> None:
        """
        Example search function that might call an external API or local index,
        then store results in the graph. Dedupe is respected if set in the constructor.
        """
        # For demonstration, we'll assume the "search" is a mock or local function.
        # In real usage, call Composio or your own search microservice.
        self.logger.debug(f"Searching for: {query_str}")
        # Mock results
        results = [f"Result for {query_str} #1", f"Result for {query_str} #2"]

        self._store_search_results(query_str, results)

    def handle_webhook(self, webhook_data: Dict[str, Any], data_source_name: str) -> None:
        """
        Ingest a webhook event into the graph. Creates a 'Webhook' node, plus an optional
        'Log' node. Auto-extraction can be triggered if enabled.
        """
        source_id = self._create_or_find_data_source(data_source_name)
        webhook_node_id = self._create_node("Webhook", webhook_data)
        self.db.create_edge(source_id, webhook_node_id, "HAS_WEBHOOK", {})

        # Store a log
        log_node_id = self._create_node("Log", {
            "timestamp": time.time(),
            "type": "webhook_event",
            "data": json.dumps(webhook_data)
        })
        self.db.create_edge(webhook_node_id, log_node_id, "HAS_LOG", {})

        # If auto_extract_structured_data is on, parse out relevant columns
        self._extract_structured_data_if_applicable(webhook_data)

    def run(self) -> None:
        """
        If a schedule_interval was set, continuously run sync or other tasks in a loop.
        Otherwise, do nothing. This is optional for environment-based scheduling.
        """
        if not self.schedule_interval:
            self.logger.info("No schedule_interval set; nothing to run in a loop.")
            return

        self.logger.info(f"Starting scheduled run loop every {self.schedule_interval} seconds.")
        while True:
            # Example: call a specific sync if you want to poll an API
            # self.sync_data("SomeAPI", action=Action.SOME_APP_FETCH_DATA, params={})
            self.logger.debug("Scheduled job iteration completed.")
            time.sleep(self.schedule_interval)
================================================================================


================================================================================
FILE: tests/conftest.py
================================================================================
"""
Pytest configuration and fixtures.
"""
import pytest
import os
import redis
from graphrouter import (
    LocalGraphDatabase,
    Neo4jGraphDatabase,
    FalkorDBGraphDatabase,
    Ontology
)

@pytest.fixture
def test_db_path(tmp_path):
    """Provide a temporary database path."""
    return str(tmp_path / "test_graph.json")

@pytest.fixture
def local_db(test_db_path):
    """Provide a local graph database instance."""
    db = LocalGraphDatabase()
    db.connect(test_db_path)
    yield db
    db.disconnect()
    if os.path.exists(test_db_path):
        os.remove(test_db_path)

@pytest.fixture
def neo4j_db():
    """Provide a Neo4j database instance."""
    db = Neo4jGraphDatabase()
    # Use environment variables for connection details
    uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
    username = os.environ.get('NEO4J_USER', 'neo4j')
    password = os.environ.get('NEO4J_PASSWORD', 'password')

    db.connect(uri, username, password)
    yield db

    # Clean up the database
    if db.connected and db.driver:
        with db.driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n")
        db.disconnect()

@pytest.fixture
def falkordb_db():
    """Provide a FalkorDB database instance."""
    db = FalkorDBGraphDatabase()
    # Use environment variables for connection details
    host = os.environ.get('FALKORDB_HOST', 'localhost')
    port = int(os.environ.get('FALKORDB_PORT', '6379'))
    password = os.environ.get('FALKORDB_PASSWORD')
    graph_name = os.environ.get('FALKORDB_GRAPH', 'test_graph')

    db.connect(host=host, port=port, password=password, graph_name=graph_name)
    yield db

    # Clean up the database
    if db.connected and db.client:
        try:
            # Attempt to see if graph exists
            graph_info = db.client.execute_command('GRAPH.QUERY', graph_name, 'RETURN 1')
            if graph_info and graph_info[1]:
                db.client.execute_command('GRAPH.DELETE', graph_name)
        except redis.exceptions.ResponseError as e:
            if "does not exist" in str(e):
                pass
            else:
                raise
    db.disconnect()

@pytest.fixture
def sample_ontology():
    """Provide a sample ontology for testing."""
    ontology = Ontology()
    # Person nodes: 'name' is required, 'age', 'email' optional
    ontology.add_node_type(
        'Person',
        {'name': 'str', 'age': 'int', 'email': 'str'},
        required=['name']
    )

    # FRIENDS_WITH edges: 'since' is required, 'strength' optional
    ontology.add_edge_type(
        'FRIENDS_WITH',
        {'since': 'str', 'strength': 'int'},
        required=['since']
    )

    # WORKS_WITH edges: 'department' is required
    ontology.add_edge_type(
        'WORKS_WITH',
        {'department': 'str'},
        required=['department']
    )

    return ontology

@pytest.fixture(params=['local', 'neo4j', 'falkordb'])
def graph_db(request, local_db, neo4j_db, falkordb_db):
    """Provide database instances for testing."""
    if request.param == 'local':
        return local_db
    elif request.param == 'neo4j':
        return neo4j_db
    return falkordb_db

================================================================================


================================================================================
FILE: tests/test_base.py
================================================================================
"""
Tests for the base GraphDatabase class functionality.
"""
import pytest
from graphrouter import GraphDatabase, LocalGraphDatabase

def test_graph_database_instantiation():
    """Test that we can instantiate a concrete implementation."""
    db = LocalGraphDatabase()
    assert isinstance(db, GraphDatabase)
    assert not db.connected

def test_connection_management(local_db):
    """Test connection management."""
    assert local_db.connected
    local_db.disconnect()
    assert not local_db.connected

def test_ontology_validation(local_db, sample_ontology):
    """Test ontology validation."""
    local_db.set_ontology(sample_ontology)
    
    # Valid node
    assert local_db.validate_node('Person', {'name': 'John', 'age': 30})
    
    # Invalid node (missing required property)
    assert not local_db.validate_node('Person', {'age': 30})
    
    # Valid edge
    assert local_db.validate_edge('FRIENDS_WITH', {'since': '2023-01-01', 'strength': 5})
    
    # Invalid edge (missing required property)
    assert not local_db.validate_edge('FRIENDS_WITH', {'strength': 5})

================================================================================


================================================================================
FILE: tests/test_cache.py
================================================================================
"""
Tests for the query cache implementation.
"""
import pytest
from datetime import datetime, timedelta
from graphrouter.cache import QueryCache

def test_cache_initialization():
    """Test cache initialization with default TTL."""
    cache = QueryCache()
    assert cache.ttl == 300  # Default TTL
    assert len(cache.cache) == 0

def test_cache_initialization_custom_ttl():
    """Test cache initialization with custom TTL."""
    cache = QueryCache(ttl=60)
    assert cache.ttl == 60

def test_cache_set_and_get():
    """Test basic cache set and get operations."""
    cache = QueryCache()
    
    # Test with different data types
    test_data = {
        'string_key': 'test_value',
        'int_key': 42,
        'dict_key': {'nested': 'data'},
        'list_key': [1, 2, 3]
    }
    
    # Set values
    for key, value in test_data.items():
        cache.set(key, value)
    
    # Get values
    for key, expected in test_data.items():
        assert cache.get(key) == expected

def test_cache_ttl():
    """Test cache TTL functionality."""
    cache = QueryCache(ttl=1)  # 1 second TTL
    
    # Set a value
    cache.set('test_key', 'test_value')
    assert cache.get('test_key') == 'test_value'
    
    # Wait for TTL to expire
    import time
    time.sleep(2)
    
    # Value should be None after TTL expiration
    assert cache.get('test_key') is None

def test_cache_overwrite():
    """Test overwriting existing cache entries."""
    cache = QueryCache()
    
    # Set initial value
    cache.set('test_key', 'initial_value')
    assert cache.get('test_key') == 'initial_value'
    
    # Overwrite value
    cache.set('test_key', 'updated_value')
    assert cache.get('test_key') == 'updated_value'

def test_cache_nonexistent_key():
    """Test getting a nonexistent key."""
    cache = QueryCache()
    assert cache.get('nonexistent_key') is None

================================================================================


================================================================================
FILE: tests/test_console.py
================================================================================

"""
Tests for console.py functionality
"""
import pytest
from unittest.mock import patch, MagicMock
import os
from graphrouter import Ontology
import console
from ingestion_engine.ingestion_engine import IngestionEngine

@pytest.fixture
def mock_engine():
    with patch('ingestion_engine.ingestion_engine.IngestionEngine') as mock:
        yield mock

@pytest.fixture
def test_ontology():
    return console.setup_ontology()

def test_setup_ontology():
    """Test ontology creation and structure"""
    ontology = console.setup_ontology()
    
    # Verify core types are present
    # Verify core types
    assert "DataSource" in ontology.node_types
    assert "File" in ontology.node_types
    assert "Row" in ontology.node_types
    assert "Log" in ontology.node_types
    
    # Verify properties
    assert "name" in ontology.node_types["DataSource"]["properties"]
    assert "file_name" in ontology.node_types["File"]["properties"]
    assert "SearchResult" in ontology.node_types
    assert "Webhook" in ontology.node_types
    
    # Verify required properties
    assert "name" in ontology.node_types["DataSource"]["required"]
    assert "file_name" in ontology.node_types["File"]["required"]
    
    # Verify relationship types
    assert "HAS_FILE" in ontology.edge_types
    assert "HAS_ROW" in ontology.edge_types
    assert "HAS_LOG" in ontology.edge_types
    assert "HAS_WEBHOOK" in ontology.edge_types
    
    # Verify relationships
    assert "HAS_FILE" in ontology.edge_types
    assert "HAS_ROW" in ontology.edge_types
    assert "HAS_LOG" in ontology.edge_types

def test_engine_initialization(tmp_path, test_ontology):
    """Test IngestionEngine initialization with proper ontology"""
    db_path = str(tmp_path / "test_graph.json")
    
    engine = IngestionEngine(
        router_config={"db_path": db_path},
        default_ontology=test_ontology,
        auto_extract_structured_data=True
    )
    
    assert engine.ontology is not None
    assert engine.db is not None

@patch('builtins.input', side_effect=['7'])  # Simulate selecting "Exit"
def test_main_exit(mock_input, capsys):
    """Test main function exits properly"""
    console.main()
    captured = capsys.readouterr()
    assert "Ingestion Engine Test Console" in captured.out
    assert "Exiting..." in captured.out

================================================================================


================================================================================
FILE: tests/test_falkordb.py
================================================================================
"""
Tests specific to the FalkorDB backend implementation.
"""
import os
import pytest
from unittest.mock import patch, MagicMock
from graphrouter import FalkorDBGraphDatabase, Query
from graphrouter.errors import ConnectionError
from graphrouter.query import AggregationType

pytestmark = pytest.mark.skipif(
    not os.environ.get("FALKORDB_HOST") or not os.environ.get("FALKORDB_PORT"),
    reason="FalkorDB credentials not available"
)

@patch('redis.Redis', autospec=True) # Added mocking for redis connection
def test_falkordb_connection(mock_redis):
    mock_conn = MagicMock()
    mock_redis.return_value = mock_conn
    db = FalkorDBGraphDatabase()
    db.connect(host="0.0.0.0", port=6379)
    assert db.connected

def test_falkordb_invalid_connection():
    db = FalkorDBGraphDatabase()
    with pytest.raises(ConnectionError):
        db.connect(host="invalid-host", port=6379)

def test_falkordb_cypher_query_generation(falkordb_db):
    query = Query()
    query.filter(Query.label_equals("Person"))
    query.filter(Query.property_equals("name", "Alice"))
    query.filter(Query.property_contains("interests", "coding"))
    query.sort("age", reverse=True)
    query.limit_results(5)

    cypher = falkordb_db._build_cypher_query(query)
    expected = (
        "MATCH (n) "
        "WHERE n:Person AND n.name = 'Alice' AND CONTAINS(n.interests, 'coding') "
        "ORDER BY n.age DESC "
        "LIMIT 5 "
        "RETURN n"
    )
    assert cypher.replace("  ", " ") == expected.replace("  ", " ")

def test_falkordb_complex_query(falkordb_db, sample_ontology):
    falkordb_db.set_ontology(sample_ontology)
    alice_id = falkordb_db.create_node("Person", {"name": "Alice", "age": 30, "interests": ["coding", "music"]})
    bob_id   = falkordb_db.create_node("Person", {"name": "Bob",   "age": 25, "interests": ["gaming", "coding"]})
    carol_id = falkordb_db.create_node("Person", {"name": "Carol", "age": 35, "interests": ["reading"]})
    falkordb_db.create_edge(alice_id, bob_id,   "FRIENDS_WITH", {"since": "2023-01-01"})
    falkordb_db.create_edge(bob_id,   carol_id, "FRIENDS_WITH", {"since": "2023-02-01"})

    query = Query()
    query.filter(Query.label_equals("Person"))
    query.filter(Query.property_contains("interests", "coding"))
    query.sort("age")
    results = falkordb_db.query(query)

    assert len(results) == 2
    assert results[0]["properties"]["name"] == "Bob"
    assert results[1]["properties"]["name"] == "Alice"

def test_falkordb_transaction_handling(falkordb_db, sample_ontology):
    falkordb_db.set_ontology(sample_ontology)
    node_id = falkordb_db.create_node("Person", {"name": "Alice"})
    assert falkordb_db.get_node(node_id) is not None

    with pytest.raises(ValueError):
        falkordb_db.create_node("Person", {"wrong_field": "value"})

    assert falkordb_db.get_node(node_id) is not None

@patch('redis.Redis')
def test_falkordb_crud_operations(mock_redis, falkordb_db):
    mock_conn = MagicMock()
    mock_redis.return_value = mock_conn

    n1 = falkordb_db.create_node("Person", {"name": "Alice", "age": 30})
    n2 = falkordb_db.create_node("Person", {"name": "Bob",   "age": 25})

    node1 = falkordb_db.get_node(n1)
    assert node1["properties"]["name"] == "Alice"

    falkordb_db.update_node(n1, {"age": 31})
    node1 = falkordb_db.get_node(n1)
    assert node1["properties"]["age"] == 31

    e_id = falkordb_db.create_edge(n1, n2, "FRIENDS_WITH", {"since": "2023-01-01"})
    edge_data = falkordb_db.get_edge(e_id)
    assert edge_data["from_id"] == n1
    assert edge_data["to_id"]   == n2

    falkordb_db.update_edge(e_id, {"strength": "close"})
    edge_data = falkordb_db.get_edge(e_id)
    assert edge_data["properties"]["strength"] == "close"

    falkordb_db.delete_node(n1)
    assert falkordb_db.get_node(n1) is None
    assert falkordb_db.get_edge(e_id) is None

def test_falkordb_error_handling(falkordb_db):
    falkordb_db.disconnect()
    with pytest.raises(ConnectionError):
        falkordb_db.create_node("Person", {"name": "Alice"})

    falkordb_db.connect()
    with pytest.raises(ValueError):
        falkordb_db.create_node("Person", None)

    node_id = falkordb_db.create_node("Person", {"name": "Alice"})
    with pytest.raises(ValueError):
        falkordb_db.create_edge(node_id, "invalid", "FRIENDS_WITH")

def test_falkordb_ontology_validation(falkordb_db, sample_ontology):
    falkordb_db.set_ontology(sample_ontology)
    n_id = falkordb_db.create_node("Person", {"name": "Alice", "age": 30})
    assert n_id is not None

    with pytest.raises(ValueError):
        falkordb_db.create_node("Person", {"age": 30})

    n2 = falkordb_db.create_node("Person", {"name": "Bob", "age": 25})
    e_id = falkordb_db.create_edge(n_id, n2, "FRIENDS_WITH", {"since": "2023-01-01", "strength": 5})
    assert e_id is not None

    with pytest.raises(ValueError):
        falkordb_db.create_edge(n_id, n2, "FRIENDS_WITH", {"strength": 5})

def test_falkordb_batch_operations(falkordb_db):
    nodes = [
        {"label": "Person", "properties": {"name": "Alice",   "age": 30}},
        {"label": "Person", "properties": {"name": "Bob",     "age": 25}},
        {"label": "Person", "properties": {"name": "Charlie", "age": 35}},
    ]
    node_ids = falkordb_db.batch_create_nodes(nodes)
    assert len(node_ids) == 3

    edges = [
        {
            "from_id": node_ids[0],
            "to_id":   node_ids[1],
            "label":   "FRIENDS_WITH",
            "properties": {"since": "2023-01-01"}
        },
        {
            "from_id": node_ids[1],
            "to_id":   node_ids[2],
            "label":   "COLLEAGUES_WITH",
            "properties": {"since": "2023-02-01"}
        }
    ]
    edge_ids = falkordb_db.batch_create_edges(edges)
    assert len(edge_ids) == 2

    for e_id in edge_ids:
        assert falkordb_db.get_edge(e_id) is not None
================================================================================


================================================================================
FILE: tests/test_ingestion_engine.py
================================================================================
"""
Tests for ingestion_engine.py

To run:
  pytest test_ingestion_engine.py
"""

import os
import pytest
import time
import json
from unittest.mock import patch
from ingestion_engine.ingestion_engine import IngestionEngine

# If you're using a local JSON-based graph DB for testing, 
# we can point to a test file.
TEST_GRAPH_DB_PATH = "test_graph.json"


@pytest.fixture
def engine(tmp_path):
    """
    Creates an instance of the IngestionEngine with a local JSON graph
    for isolated testing.
    """
    # Point to a temp path for the local graph DB
    test_db_file = tmp_path / "test_graph.json"
    router_config = {
        "db_path": str(test_db_file)
    }

    # Fake composio config (for demonstration)
    composio_config = {
        "api_key": "TEST_API_KEY",
        "entity_id": "TestEntity"
    }

    engine = IngestionEngine(
        router_config=router_config,
        composio_config=composio_config,
        auto_extract_structured_data=True,
        extraction_rules={
            "include_columns": ["timestamp", "message"]
        },
        deduplicate_search_results=True,
        schedule_interval=None  # We'll not run the loop in tests
    )

    return engine

def test_file_upload_csv(engine, tmp_path):
    # Create a mock CSV
    csv_file = tmp_path / "test.csv"
    with open(csv_file, mode="w", encoding="utf-8") as f:
        f.write("id,name\n1,TestUser\n2,AnotherUser\n")

    file_node_id = engine.upload_file(str(csv_file), "LocalTestSource", parse_csv=True)
    assert file_node_id is not None, "Should create a File node for the CSV"

    # Verify Row nodes were created and linked
    query = engine.db.create_query()
    query.filter(Query.label_equals("Row"))
    rows = engine.db.query(query)
    assert len(rows) == 2, "Should create 2 Row nodes"

    # Verify links to File node
    query = engine.db.create_query()
    query.filter(Query.relationship_exists(file_node_id, "HAS_ROW"))
    links = engine.db.query(query)
    assert len(links) == 2, "Should create 2 HAS_ROW relationships"

def test_file_upload_non_csv(engine, tmp_path):
    # Create a mock text file
    txt_file = tmp_path / "test.txt"
    with open(txt_file, mode="w", encoding="utf-8") as f:
        f.write("Hello, world")

    file_node_id = engine.upload_file(str(txt_file), "LocalTestSource", parse_csv=True)
    assert file_node_id is not None

def test_download_data(engine):
    # We'll mock or stub composio_toolset usage
    # If the real composio is configured, 
    # you'd use something like Action.GITHUB_GET_CONTENTS_OF_A_REPOSITORY etc.
    if not engine.composio_toolset:
        pytest.skip("No composio installed or configured")

    # Example: letâ€™s call the function with a dummy action
    # (In practice, youâ€™d define valid action IDs from Composio)
    data = engine.download_data(action="MOCK_DOWNLOAD_DATA", params={"fake": True})
    # Check that we stored a log node
    assert "result" in data or "fake" in data

def test_sync_data(engine):
    if not engine.composio_toolset:
        pytest.skip("No composio installed or configured")

    # Example usage for syncing
    engine.sync_data("SampleAPI", action="MOCK_SYNC_DATA", params={"page": 1})
    # Potentially check the graph for a "Log" node with 'type=sync' 
    # or something similar.

def test_search_and_store_results(engine):
    query_string = "test query"
    engine.search_and_store_results(query_string)
    # Validate that search results have been stored 
    # as 'SearchResult' nodes in the graph DB
    # If needed, parse the local JSON or run a query using engine.db

def test_handle_webhook(engine):
    webhook_data = {
        "event": "UserSignup",
        "user_id": 123,
        "timestamp": time.time(),
        "debug_info": "Some internal stuff"
    }
    engine.handle_webhook(webhook_data, "WebhookSource")

    # Verify Webhook node creation
    query = engine.db.create_query()
    query.filter(Query.label_equals("Webhook"))
    webhook_nodes = engine.db.query(query)
    assert len(webhook_nodes) == 1, "Should create 1 Webhook node"

    # Verify Log node creation and linking
    query = engine.db.create_query()
    query.filter(Query.label_equals("Log"))
    query.filter(Query.property_equals("type", "webhook_event"))
    log_nodes = engine.db.query(query)
    assert len(log_nodes) == 1, "Should create 1 Log node"

@patch('llm_engine.node_processor.NodeProcessor')
def test_llm_enrichment(mock_processor, engine):
    mock_llm = MagicMock()
    engine_with_llm = IngestionEngine(
        router_config={"db_path": "test_graph.json"},
        llm_integration=mock_llm,
        auto_extract_structured_data=True
    )

    # Test node processing with LLM
    data = {"content": "Test content for LLM processing"}
    node_id = engine_with_llm._create_node("Document", data)

    # Verify LLM integration was called
    assert hasattr(engine_with_llm, "node_processor")
    assert hasattr(engine_with_llm, "enrichment_manager")
================================================================================


================================================================================
FILE: tests/test_litellm_client.py
================================================================================
"""
test_litellm_client.py

Tests for LiteLLMClient in litellm_client.py,
including both mock-based unit tests and optional integration tests.
"""

import os
import json
import pytest
from unittest.mock import patch, MagicMock
import os
from llm_engine.litellm_client import LiteLLMClient

from llm_engine.litellm_client import LiteLLMClient, LiteLLMError

##############################################################################
# Optional fixture for real integration tests
##############################################################################
@pytest.fixture(scope="session")
def openai_api_key():
    key = os.environ.get("OPENAI_API_KEY") or os.environ.get("GH_OPENAI_API_KEY")
    if not key:
        pytest.skip("No OpenAI API key found, skipping real integration tests.")
    return key


##############################################################################
# Mock-based (unit) Tests
##############################################################################

@patch("llm_engine.litellm_client.litellm.chat_completion", autospec=True)
def test_call_structured_valid_json(mock_chat):
    """
    Test call_structured with valid JSON returned by mocked chat_completion.
    """
    # Mock the response from chat_completion
    mock_chat.return_value = {"content": json.dumps({"title": "Hello", "score": 42})}

    client = LiteLLMClient(api_key="FAKE_KEY")
    schema = {"title": "string", "score": "number"}

    prompt = "Give me a JSON with title and score."
    result = client.call_structured(prompt, schema)
    assert result == {"title": "Hello", "score": 42}


@patch("llm_engine.litellm_client.litellm.chat_completion", autospec=True)
def test_call_structured_invalid_json(mock_chat):
    """
    Test call_structured raises LiteLLMError if chat returns invalid JSON.
    """
    mock_chat.return_value = {"content": "NOT VALID JSON"}

    client = LiteLLMClient(api_key="FAKE_KEY")
    schema = {"title": "string"}

    with pytest.raises(LiteLLMError) as excinfo:
        client.call_structured("Prompt", schema)

    assert "JSON parse error" in str(excinfo.value)


@patch("llm_engine.litellm_client.litellm.chat_completion", autospec=True)
def test_call_structured_exception_in_chat(mock_chat):
    """
    Test call_structured raises LiteLLMError if chat_completion throws an exception.
    """
    mock_chat.side_effect = Exception("Some internal error")

    client = LiteLLMClient(api_key="FAKE_KEY")
    with pytest.raises(LiteLLMError) as excinfo:
        client.call_structured("Prompt text", {"title": "string"})

    assert "Error during LLM call" in str(excinfo.value)


@patch("llm_engine.litellm_client.litellm.embedding", autospec=True)
def test_get_embedding_success(mock_embedding):
    """
    Test get_embedding returns the embedding from litellm.embedding successfully.
    """
    mock_embedding.return_value = [0.1, 0.2, 0.3]

    client = LiteLLMClient(api_key="FAKE_KEY")
    vec = client.get_embedding("Hello world")
    assert vec == [0.1, 0.2, 0.3]


@patch("llm_engine.litellm_client.litellm.embedding", autospec=True)
def test_get_embedding_error(mock_embedding):
    """
    Test get_embedding raises LiteLLMError if embedding call fails.
    """
    mock_embedding.side_effect = Exception("Embedding Error")

    client = LiteLLMClient(api_key="FAKE_KEY")
    with pytest.raises(LiteLLMError) as excinfo:
        client.get_embedding("Some text")

    assert "Error retrieving embedding" in str(excinfo.value)


##############################################################################
# Integration Tests (real calls)
##############################################################################
@pytest.mark.integration
def test_call_structured_integration(openai_api_key):
    """
    Integration test: actually call litellm.chat_completion
    using your real OPENAI_API_KEY.
    If no key is found, test is skipped.
    """
    client = LiteLLMClient(
        api_key=openai_api_key,
        model_name="gpt-3.5-turbo",
        temperature=0.0,
        max_tokens=100
    )

    schema = {"name": "string", "age": "number"}
    prompt = "Return only valid JSON with name and age keys."
    result = client.call_structured(prompt, schema)
    print("Integration structured result:", result)
    assert "name" in result
    assert "age" in result


@pytest.mark.integration
def test_get_embedding_integration(openai_api_key):
    """
    Integration test: actually call litellm.embedding with a real OpenAI key.
    """
    client = LiteLLMClient(
        api_key=openai_api_key,
        model_name="text-embedding-ada-002"
    )

    text = "GraphRouter is a flexible Python library for graph databases."
    embedding = client.get_embedding(text)
    print("Integration embedding length:", len(embedding))

    assert isinstance(embedding, list)
    assert len(embedding) > 10
    assert all(isinstance(v, float) for v in embedding)
================================================================================


================================================================================
FILE: tests/test_local.py
================================================================================
"""
Tests for the LocalGraphDatabase implementation with advanced query features.
"""
import pytest
from graphrouter import LocalGraphDatabase, Query
from graphrouter.query import AggregationType
from graphrouter.errors import ConnectionError
from graphrouter.ontology import Ontology  # Assuming Ontology class is available

def test_advanced_query_operations(local_db, sample_ontology):
    """Test advanced query operations including path finding and aggregations."""
    # Set ontology
    local_db.set_ontology(sample_ontology)

    # Create test data - social network
    alice = local_db.create_node('Person', {'name': 'Alice', 'age': 30})
    bob = local_db.create_node('Person', {'name': 'Bob', 'age': 25})
    charlie = local_db.create_node('Person', {'name': 'Charlie', 'age': 35})
    david = local_db.create_node('Person', {'name': 'David', 'age': 28})

    # Create relationships
    edge1 = local_db.create_edge(alice, bob, 'FRIENDS_WITH', {'since': '2023-01'})
    edge2 = local_db.create_edge(bob, charlie, 'WORKS_WITH', {'department': 'IT'})
    edge3 = local_db.create_edge(charlie, david, 'FRIENDS_WITH', {'since': '2023-02'})

    # Test path finding
    query = Query()
    query.find_path('Person', 'Person', ['FRIENDS_WITH', 'WORKS_WITH'], min_depth=1, max_depth=2)
    paths = local_db.query(query)

    assert len(paths) > 0
    # Verify path from Alice to Charlie exists
    path_exists = any(
        p['start_node']['properties']['name'] == 'Alice' and
        p['end_node']['properties']['name'] == 'Charlie'
        for p in paths
    )
    assert path_exists

    # Test relationship filtering
    query = Query()
    query.find_path('Person', 'Person', ['FRIENDS_WITH'])
    query.filter_relationship(
        lambda r: r.get('properties', {}).get('since', '').startswith('2023')
    )
    results = local_db.query(query)
    assert len(results) > 0
    assert all(r['relationships'] and r['relationships'][0]['label'] == 'FRIENDS_WITH' for r in results)

    # Test aggregations
    query = Query()
    query.filter(Query.label_equals('Person'))
    query.aggregate(AggregationType.AVG, 'age', 'avg_age')
    query.aggregate(AggregationType.COUNT, alias='total_people')
    results = local_db.query(query)

    assert len(results) == 1
    aggs = results[0]
    assert abs(aggs['avg_age'] - 29.5) < 0.01  # Average of 30, 25, 35, 28
    assert aggs['total_people'] == 4

def test_query_stats(local_db):
    """Test query execution statistics."""
    # Create test data
    for i in range(5):
        local_db.create_node('TestNode', {'index': i})

    query = Query()
    query.filter(Query.label_equals('TestNode'))
    results = local_db.query(query)

    stats = query.collect_stats()
    assert stats['nodes_scanned'] == 5
    assert stats['execution_time'] > 0
    assert stats['memory_used'] > 0

def test_pagination(local_db):
    """Test query pagination."""
    # Create test data
    node_ids = []
    for i in range(10):
        node_id = local_db.create_node('TestNode', {'index': i})
        node_ids.append(node_id)

    # Test first page
    query = Query()
    query.filter(Query.label_equals('TestNode'))
    query.sort('index')
    query.paginate(page=1, page_size=3)
    results = local_db.query(query)

    assert len(results) == 3
    assert results[0]['properties']['index'] == 0
    assert results[2]['properties']['index'] == 2

    # Test second page
    query = Query()
    query.filter(Query.label_equals('TestNode'))
    query.sort('index')
    query.paginate(page=2, page_size=3)
    results = local_db.query(query)

    assert len(results) == 3
    assert results[0]['properties']['index'] == 3
    assert results[2]['properties']['index'] == 5

def test_query_operations(local_db):
    """Test query operations."""
    # Create test data
    local_db.create_node('Person', {'name': 'Alice', 'age': 30})
    local_db.create_node('Person', {'name': 'Bob', 'age': 25})
    local_db.create_node('Person', {'name': 'Charlie', 'age': 35})

    # Test query filters
    query = Query()
    query.filter(Query.label_equals('Person'))
    query.filter(Query.property_equals('age', 30))
    results = local_db.query(query)
    assert len(results) == 1
    assert results[0]['properties']['name'] == 'Alice'

    # Test sorting
    query = Query()
    query.filter(Query.label_equals('Person'))
    query.sort('age', reverse=True)
    results = local_db.query(query)
    assert len(results) == 3
    assert results[0]['properties']['name'] == 'Charlie'
    assert results[2]['properties']['name'] == 'Bob'

    # Test limit
    query = Query()
    query.filter(Query.label_equals('Person'))
    query.sort('age')
    query.limit_results(2)
    results = local_db.query(query)
    assert len(results) == 2
    assert results[0]['properties']['name'] == 'Bob'

def test_persistence(local_db, test_db_path):
    """Test database persistence."""
    # Create test data
    node_id = local_db.create_node('Person', {'name': 'Alice'})

    # Disconnect and reconnect
    local_db.disconnect()
    local_db.connect(test_db_path)

    # Verify data persisted
    node = local_db.get_node(node_id)
    assert node['properties']['name'] == 'Alice'

def test_crud_operations(local_db):
    """Test basic CRUD operations."""
    # Create nodes
    node1_id = local_db.create_node('Person', {'name': 'Alice', 'age': 30})
    node2_id = local_db.create_node('Person', {'name': 'Bob', 'age': 25})

    # Verify nodes
    node1 = local_db.get_node(node1_id)
    assert node1['properties']['name'] == 'Alice'
    assert node1['properties']['age'] == 30

    # Update node
    local_db.update_node(node1_id, {'age': 31})
    node1 = local_db.get_node(node1_id)
    assert node1['properties']['age'] == 31

    # Create edge
    edge_id = local_db.create_edge(
        node1_id,
        node2_id,
        'FRIENDS_WITH',
        {'since': '2023-01-01'}
    )

    # Verify edge
    edge = local_db.get_edge(edge_id)
    assert edge['from_id'] == node1_id
    assert edge['to_id'] == node2_id

    # Update edge
    local_db.update_edge(edge_id, {'strength': 'close'})
    edge = local_db.get_edge(edge_id)
    assert edge['properties']['strength'] == 'close'

    # Delete node (should also delete connected edges)
    local_db.delete_node(node1_id)
    assert local_db.get_node(node1_id) is None
    assert local_db.get_edge(edge_id) is None

def test_batch_operations(local_db):
    """Test batch creation operations."""
    # Test batch node creation
    nodes = [
        {'label': 'Person', 'properties': {'name': 'Alice', 'age': 30}},
        {'label': 'Person', 'properties': {'name': 'Bob', 'age': 25}},
        {'label': 'Person', 'properties': {'name': 'Charlie', 'age': 35}}
    ]

    node_ids = local_db.batch_create_nodes(nodes)
    assert len(node_ids) == 3

    for node_id in node_ids:
        assert local_db.get_node(node_id) is not None

    # Test batch edge creation
    edges = [
        {
            'from_id': node_ids[0],
            'to_id': node_ids[1],
            'label': 'FRIENDS_WITH',
            'properties': {'since': '2023-01-01'}
        },
        {
            'from_id': node_ids[1],
            'to_id': node_ids[2],
            'label': 'COLLEAGUES_WITH',
            'properties': {'since': '2023-02-01'}
        }
    ]

    edge_ids = local_db.batch_create_edges(edges)
    assert len(edge_ids) == 2

    for edge_id in edge_ids:
        assert local_db.get_edge(edge_id) is not None

def test_error_handling(local_db):
    """Test error handling."""
    # Test disconnected operations
    local_db.disconnect()
    with pytest.raises(ConnectionError):
        local_db.create_node('Person', {'name': 'Alice'})

    # Test invalid node creation
    local_db.connect()
    with pytest.raises(ValueError):
        local_db.create_node('Person', None)

    # Test invalid edge creation
    node_id = local_db.create_node('Person', {'name': 'Alice'})
    with pytest.raises(ValueError):
        local_db.create_edge(node_id, 'invalid_id', 'FRIENDS_WITH')

def test_ontology_validation(local_db, sample_ontology):
    """Test ontology validation."""
    local_db.set_ontology(sample_ontology)

    # Test valid node creation
    node_id = local_db.create_node('Person', {'name': 'Alice', 'age': 30})
    assert node_id is not None

    # Test invalid node creation (missing required property)
    with pytest.raises(ValueError):
        local_db.create_node('Person', {'age': 30})

    # Test valid edge creation
    node2_id = local_db.create_node('Person', {'name': 'Bob', 'age': 25})
    edge_id = local_db.create_edge(
        node_id,
        node2_id,
        'FRIENDS_WITH',
        {'since': '2023-01-01', 'strength': 5}
    )
    assert edge_id is not None

    # Test invalid edge creation (missing required property)
    with pytest.raises(ValueError):
        local_db.create_edge(node_id, node2_id, 'FRIENDS_WITH', {'strength': 5})

def test_batch_validation(local_db, sample_ontology):
    """Test batch operations with ontology validation."""
    local_db.set_ontology(sample_ontology)

    # Test valid batch node creation
    valid_nodes = [
        {'label': 'Person', 'properties': {'name': 'Alice', 'age': 30}},
        {'label': 'Person', 'properties': {'name': 'Bob', 'age': 25}}
    ]
    node_ids = local_db.batch_create_nodes(valid_nodes)
    assert len(node_ids) == 2

    # Test invalid batch node creation
    invalid_nodes = [
        {'label': 'Person', 'properties': {'name': 'Charlie'}},
        {'label': 'Person', 'properties': {'age': 35}}  # Missing required 'name'
    ]
    with pytest.raises(ValueError):
        local_db.batch_create_nodes(invalid_nodes)

    # Test valid batch edge creation
    valid_edges = [
        {
            'from_id': node_ids[0],
            'to_id': node_ids[1],
            'label': 'FRIENDS_WITH',
            'properties': {'since': '2023-01-01', 'strength': 5}
        }
    ]
    edge_ids = local_db.batch_create_edges(valid_edges)
    assert len(edge_ids) == 1

    # Test invalid batch edge creation
    invalid_edges = [
        {
            'from_id': node_ids[0],
            'to_id': node_ids[1],
            'label': 'FRIENDS_WITH',
            'properties': {'strength': 5}  # Missing required 'since'
        }
    ]
    with pytest.raises(ValueError):
        local_db.batch_create_edges(invalid_edges)

================================================================================


================================================================================
FILE: tests/test_monitoring.py
================================================================================
"""
Tests for the performance monitoring implementation.
"""
import pytest
from graphrouter.monitoring import PerformanceMonitor

def test_monitor_initialization():
    """Test monitor initialization."""
    monitor = PerformanceMonitor()
    assert len(monitor.metrics) == 0

def test_record_operation():
    """Test recording operation metrics."""
    monitor = PerformanceMonitor()
    
    # Record some test operations
    monitor.record_operation('query', 0.5)
    monitor.record_operation('query', 1.0)
    monitor.record_operation('create_node', 0.3)
    
    # Verify metrics were recorded
    assert len(monitor.metrics['query']) == 2
    assert len(monitor.metrics['create_node']) == 1

def test_get_average_times():
    """Test calculating average operation times."""
    monitor = PerformanceMonitor()
    
    # Record multiple operations
    monitor.record_operation('query', 1.0)
    monitor.record_operation('query', 2.0)
    monitor.record_operation('query', 3.0)
    monitor.record_operation('create_node', 0.5)
    monitor.record_operation('create_node', 1.5)
    
    averages = monitor.get_average_times()
    assert averages['query'] == 2.0  # (1 + 2 + 3) / 3
    assert averages['create_node'] == 1.0  # (0.5 + 1.5) / 2

def test_reset_metrics():
    """Test resetting metrics."""
    monitor = PerformanceMonitor()
    
    # Record some operations
    monitor.record_operation('query', 1.0)
    monitor.record_operation('create_node', 0.5)
    
    # Verify metrics exist
    assert len(monitor.metrics) > 0
    
    # Reset metrics
    monitor.reset()
    
    # Verify metrics were cleared
    assert len(monitor.metrics) == 0
    assert monitor.get_average_times() == {}

def test_multiple_operation_types():
    """Test handling multiple operation types."""
    monitor = PerformanceMonitor()
    
    operations = {
        'query': [0.5, 1.0, 1.5],
        'create_node': [0.2, 0.3],
        'update_node': [0.4],
        'delete_node': [0.6, 0.8]
    }
    
    # Record operations
    for op_type, times in operations.items():
        for time in times:
            monitor.record_operation(op_type, time)
    
    # Verify all operation types were recorded
    averages = monitor.get_average_times()
    assert len(averages) == len(operations)
    assert abs(averages['query'] - 1.0) < 0.001  # (0.5 + 1.0 + 1.5) / 3
    assert abs(averages['create_node'] - 0.25) < 0.001  # (0.2 + 0.3) / 2
    assert abs(averages['update_node'] - 0.4) < 0.001
    assert abs(averages['delete_node'] - 0.7) < 0.001  # (0.6 + 0.8) / 2

================================================================================


================================================================================
FILE: tests/test_neo4j.py
================================================================================
"""
Tests specific to the Neo4j backend implementation.
"""
import os
import pytest
from graphrouter import Neo4jGraphDatabase, Query
from graphrouter.errors import ConnectionError
from graphrouter.query import AggregationType
from unittest.mock import patch, MagicMock

pytestmark = pytest.mark.skipif(
    "NEO4J_URI" not in os.environ,
    reason="Neo4j credentials not available"
)

@patch('neo4j.GraphDatabase', autospec=True)
def test_neo4j_connection(mock_neo4j):
    mock_driver = MagicMock()
    mock_neo4j.driver.return_value = mock_driver
    db = Neo4jGraphDatabase()
    db.connect(uri="bolt://0.0.0.0:7687", username="neo4j", password="password")
    assert db.is_connected()

def test_neo4j_invalid_connection():
    db = Neo4jGraphDatabase()
    with pytest.raises(ConnectionError):
        db.connect("bolt://invalid:7687", "neo4j", "wrong")

def test_neo4j_cypher_query_generation(neo4j_db):
    query = Query()
    query.filter(Query.label_equals("Person"))
    query.filter(Query.property_equals("name", "Alice"))
    query.filter(Query.property_contains("interests", "coding"))
    query.sort("age", reverse=True)
    query.limit_results(5)

    cypher = neo4j_db._build_cypher_query(query)
    expected = (
        "MATCH (n) "
        "WHERE n:Person AND n.name = 'Alice' AND n.interests CONTAINS 'coding' "
        "ORDER BY n.age DESC "
        "LIMIT 5 "
        "RETURN n"
    )
    assert cypher.replace("  ", " ") == expected.replace("  ", " ")

def test_neo4j_complex_query(neo4j_db, sample_ontology):
    neo4j_db.set_ontology(sample_ontology)
    alice_id = neo4j_db.create_node("Person", {"name": "Alice",   "age": 30, "interests": ["coding", "music"]})
    bob_id   = neo4j_db.create_node("Person", {"name": "Bob",     "age": 25, "interests": ["gaming", "coding"]})
    carol_id = neo4j_db.create_node("Person", {"name": "Carol",   "age": 35, "interests": ["reading"]})
    neo4j_db.create_edge(alice_id, bob_id,   "FRIENDS_WITH", {"since": "2023-01-01"})
    neo4j_db.create_edge(bob_id,   carol_id, "FRIENDS_WITH", {"since": "2023-02-01"})

    query = Query()
    query.filter(Query.label_equals("Person"))
    query.filter(Query.property_contains("interests", "coding"))
    query.sort("age")
    results = neo4j_db.query(query)

    assert len(results) == 2
    assert results[0]["properties"]["name"] == "Bob"
    assert results[1]["properties"]["name"] == "Alice"

def test_neo4j_transaction_handling(neo4j_db):
    node_id = neo4j_db.create_node("Person", {"name": "Alice"})
    assert neo4j_db.get_node(node_id) is not None

    with pytest.raises(ValueError):
        neo4j_db.create_node("Person", {"wrong_field": "value"})

    assert neo4j_db.get_node(node_id) is not None

def test_neo4j_crud_operations(neo4j_db):
    n1 = neo4j_db.create_node("Person", {"name": "Alice", "age": 30})
    n2 = neo4j_db.create_node("Person", {"name": "Bob",   "age": 25})

    node1 = neo4j_db.get_node(n1)
    assert node1["properties"]["name"] == "Alice"

    neo4j_db.update_node(n1, {"age": 31})
    node1 = neo4j_db.get_node(n1)
    assert node1["properties"]["age"] == 31

    e_id = neo4j_db.create_edge(n1, n2, "FRIENDS_WITH", {"since": "2023-01-01"})
    edge_data = neo4j_db.get_edge(e_id)
    assert edge_data["from_id"] == n1
    assert edge_data["to_id"]   == n2

    neo4j_db.update_edge(e_id, {"strength": "close"})
    edge_data = neo4j_db.get_edge(e_id)
    assert edge_data["properties"]["strength"] == "close"

    neo4j_db.delete_node(n1)
    assert neo4j_db.get_node(n1) is None
    assert neo4j_db.get_edge(e_id) is None

def test_neo4j_error_handling(neo4j_db):
    neo4j_db.disconnect()
    with pytest.raises(ConnectionError):
        neo4j_db.create_node("Person", {"name": "Alice"})

    # reconnect
    import os
    uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
    user = os.environ.get('NEO4J_USER', 'neo4j')
    pwd  = os.environ.get('NEO4J_PASSWORD', 'password')
    neo4j_db.connect(uri, user, pwd)

    with pytest.raises(ValueError):
        neo4j_db.create_node("Person", None)

    node_id = neo4j_db.create_node("Person", {"name": "Alice"})
    with pytest.raises(ValueError):
        neo4j_db.create_edge(node_id, "invalid", "FRIENDS_WITH")

def test_neo4j_ontology_validation(neo4j_db, sample_ontology):
    neo4j_db.set_ontology(sample_ontology)
    n_id = neo4j_db.create_node("Person", {"name": "Alice", "age": 30})
    assert n_id is not None

    with pytest.raises(ValueError):
        neo4j_db.create_node("Person", {"age": 30})

    n2 = neo4j_db.create_node("Person", {"name": "Bob", "age": 25})
    e_id = neo4j_db.create_edge(n_id, n2, "FRIENDS_WITH", {"since": "2023-01-01", "strength": 5})
    assert e_id is not None

    with pytest.raises(ValueError):
        neo4j_db.create_edge(n_id, n2, "FRIENDS_WITH", {"strength": 5})

def test_neo4j_batch_operations(neo4j_db):
    nodes = [
        {"label": "Person", "properties": {"name": "Alice",   "age": 30}},
        {"label": "Person", "properties": {"name": "Bob",     "age": 25}},
        {"label": "Person", "properties": {"name": "Charlie", "age": 35}},
    ]
    node_ids = neo4j_db.batch_create_nodes(nodes)
    assert len(node_ids) == 3

    edges = [
        {
            "from_id": node_ids[0],
            "to_id":   node_ids[1],
            "label":   "FRIENDS_WITH",
            "properties": {"since": "2023-01-01"}
        },
        {
            "from_id": node_ids[1],
            "to_id":   node_ids[2],
            "label":   "COLLEAGUES_WITH",
            "properties": {"since": "2023-02-01"}
        }
    ]
    edge_ids = neo4j_db.batch_create_edges(edges)
    assert len(edge_ids) == 2

    for e_id in edge_ids:
        e_data = neo4j_db.get_edge(e_id)
        assert e_data is not None

# NOTE: The provided changes are incomplete.  Mocking should be applied to all tests interacting with the Neo4j database for a complete solution.
================================================================================


================================================================================
FILE: tests/test_node_processor.py
================================================================================
"""
Tests for node_processor.py

To run:
    pytest test_node_processor.py
"""

import pytest
from llm_engine.node_processor import NodeProcessor, ExtractionRule

class MockLLMIntegration:
    def __init__(self):
        self.db = MockDB()
        self.extraction_calls = []

    def structured_extraction_for_node(self, text, schema, node_label):
        self.extraction_calls.append((text, schema, node_label))
        if "company" in text.lower():
            return {
                "nodes": [
                    {"label": "Person", "properties": {"name": "Alice", "role": "CEO"}},
                    {"label": "Company", "properties": {"name": "TechCorp"}}
                ],
                "relationships": [
                    {"from": "Person", "to": "Company", "type": "WORKS_AT", "properties": {}}
                ]
            }
        return {"name": "Test", "role": "Developer"}

class MockDB:
    def __init__(self):
        self.nodes = {}
        self.edges = []
        self.node_counter = 0
        self.ontology = None

    def create_node(self, label, properties):
        self.node_counter += 1
        node_id = f"node_{self.node_counter}"
        self.nodes[node_id] = {"label": label, "properties": properties}
        return node_id

    def update_node(self, node_id, properties):
        if node_id in self.nodes:
            self.nodes[node_id]["properties"].update(properties)

    def create_edge(self, from_id, to_id, rel_type, properties):
        self.edges.append({
            "from": from_id,
            "to": to_id,
            "type": rel_type,
            "properties": properties
        })

@pytest.fixture
def llm_integration():
    return MockLLMIntegration()

@pytest.fixture
def processor(llm_integration):
    return NodeProcessor(llm_integration)

def test_register_rule(processor):
    rule = ExtractionRule(
        node_label="Person",
        extract_nodes=True,
        extract_properties=True
    )
    processor.register_rule(rule)
    assert "Person" in processor.rules
    assert processor.rules["Person"] == rule

def test_process_node_property_extraction(processor):
    rule = ExtractionRule(
        node_label="Person",
        extract_properties=True,
        target_schema={"name": "", "role": ""},
        overwrite_existing=True
    )
    processor.register_rule(rule)

    node_data = {
        "label": "Person",
        "properties": {
            "content": "Test person works as a developer"
        }
    }

    processor.process_node("test_node", node_data)
    assert processor.llm_integration.extraction_calls[0][0] == "Test person works as a developer"
    assert "name" in processor.llm_integration.extraction_calls[0][1]
    assert processor.llm_integration.db.nodes["test_node"]["properties"]["role"] == "Developer"

def test_process_node_multi_extraction(processor):
    rule = ExtractionRule(
        node_label="Person",
        extract_nodes=True,
        multi_node_types=["Company"],
        relationship_types=["WORKS_AT"],
        extract_properties=True
    )
    processor.register_rule(rule)

    node_data = {
        "label": "Person",
        "properties": {
            "content": "Alice is the CEO of TechCorp"
        }
    }

    processor.process_node("test_node", node_data)
    assert len(processor.llm_integration.db.nodes) == 2  # Person and Company nodes
    assert len(processor.llm_integration.db.edges) == 1  # WORKS_AT relationship

def test_process_node_selective_params(processor):
    rule = ExtractionRule(
        node_label="Person",
        extract_properties=True,
        target_schema={"name": "", "role": "", "age": 0},
        extract_params=["name", "role"],
        overwrite_existing=False
    )
    processor.register_rule(rule)

    node_data = {
        "label": "Person",
        "properties": {
            "content": "Test content",
            "age": 25
        }
    }

    processor.process_node("test_node", node_data)
    final_props = processor.llm_integration.db.nodes["test_node"]["properties"]
    assert "age" in final_props
    assert final_props["age"] == 25  # Original value preserved

def test_process_node_property_triggers(processor):
    rule = ExtractionRule(
        node_label="Person",
        extract_properties=True,
        property_triggers=["needs_processing"],
        target_schema={"name": "", "role": ""}
    )
    processor.register_rule(rule)

    # Node without trigger property
    node_data = {
        "label": "Person",
        "properties": {
            "content": "Test content"
        }
    }

    processor.process_node("test_node", node_data)
    assert len(processor.llm_integration.extraction_calls) == 0

    # Node with trigger property
    node_data["properties"]["needs_processing"] = True
    processor.process_node("test_node", node_data)
    assert len(processor.llm_integration.extraction_calls) == 1

def test_invalid_relationship_type(processor):
    rule = ExtractionRule(
        node_label="Person",
        extract_nodes=True,
        relationship_types=["VALID_REL"]
    )
    processor.register_rule(rule)

    node_data = {
        "label": "Person",
        "properties": {
            "content": "Alice is the CEO of TechCorp"
        }
    }

    with pytest.raises(ValueError, match="Invalid relationship type: WORKS_AT"):
        processor.process_node("test_node", node_data)
================================================================================


================================================================================
FILE: tests/test_ontology.py
================================================================================
"""
Tests for the Ontology management system.
"""
import pytest
from graphrouter import Ontology
from graphrouter.errors import InvalidPropertyError, InvalidNodeTypeError

def test_ontology_creation():
    """Test ontology creation and basic operations."""
    ontology = Ontology()

    # Add node type
    ontology.add_node_type(
        'Person',
        {'name': 'str', 'age': 'int'},
        required=['name']
    )

    # Add edge type
    ontology.add_edge_type(
        'KNOWS',
        {'since': 'str'},
        required=['since']
    )

    # Verify structure
    assert 'Person' in ontology.node_types
    assert 'KNOWS' in ontology.edge_types

def test_node_validation(sample_ontology):
    """Test node validation."""
    # Valid node
    assert sample_ontology.validate_node('Person', {
        'name': 'Alice',
        'age': 30,
        'email': 'alice@example.com'
    })

    # Invalid node (wrong property type)
    with pytest.raises(InvalidPropertyError):
        sample_ontology.validate_node('Person', {
            'name': 'Alice',
            'age': '30'  # Should be int
        })

    # Invalid node (missing required property)
    with pytest.raises(InvalidPropertyError):
        sample_ontology.validate_node('Person', {
            'age': 30
        })

def test_edge_validation(sample_ontology):
    """Test edge validation."""
    # Valid edge
    assert sample_ontology.validate_edge('FRIENDS_WITH', {
        'since': '2023-01-01',
        'strength': 5
    })

    # Invalid edge (missing required property)
    with pytest.raises(InvalidPropertyError):
        sample_ontology.validate_edge('FRIENDS_WITH', {
            'strength': 5
        })

def test_ontology_serialization(sample_ontology):
    """Test ontology serialization."""
    # Convert to dict
    ontology_dict = sample_ontology.to_dict()

    # Create new ontology from dict
    new_ontology = Ontology.from_dict(ontology_dict)

    # Verify structure is preserved
    assert new_ontology.node_types == sample_ontology.node_types
    assert new_ontology.edge_types == sample_ontology.edge_types

================================================================================


================================================================================
FILE: tests/test_tool_integration.py
================================================================================
"""
test_tool_integration.py

Unit tests for tool_integration.py
"""

import pytest
from unittest.mock import MagicMock

# Adjust import paths to match your layout
from llm_engine.tool_integration import LLMToolIntegration
from llm_engine.litellm_client import LiteLLMClient, LiteLLMError
from graphrouter import GraphDatabase, Query


class MockGraphDatabase(GraphDatabase):
    """A mock GraphDatabase for testing (fills in abstract methods)."""
    def connect(self, **kwargs) -> bool:
        self.connected = True
        return True

    def disconnect(self) -> bool:
        self.connected = False
        return True

    def _create_node_impl(self, label, properties):
        # Return a dummy ID
        return f"{label}_123"

    def _get_node_impl(self, node_id):
        # Return dummy data to simulate a node
        if "Person" in node_id:
            return {
                "label": "Person",
                "properties": {
                    "name": "Alice",
                    "description": "Some info",
                    "embedding": None
                }
            }
        return None

    def _update_node_impl(self, node_id, properties):
        return True

    def _delete_node_impl(self, node_id):
        return True

    def _create_edge_impl(self, from_id, to_id, label, properties):
        return "edge_123"

    def _get_edge_impl(self, edge_id):
        return None

    def _update_edge_impl(self, edge_id, properties):
        return True

    def _delete_edge_impl(self, edge_id):
        return True

    # MISSING BATCH IMPLEMENTATIONS
    def _batch_create_nodes_impl(self, nodes):
        return [f"{n['label']}_{i}" for i, n in enumerate(nodes)]

    def _batch_create_edges_impl(self, edges):
        return [f"edge_{i}" for i, e in enumerate(edges)]

    def _query_impl(self, query: Query):
        # Return a small set of mock nodes for testing
        return [
            {"id": "Person_1", "label": "Person", "properties": {"name": "Alice"}},
            {"id": "Person_2", "label": "Person", "properties": {"name": "Bob", "embedding": [0.1, 0.2]}},
        ]


@pytest.fixture
def mock_db():
    """Fixture to provide a connected mock GraphDatabase."""
    db = MockGraphDatabase()
    db.connect()
    return db


@pytest.fixture
def mock_llm_client():
    """
    Provide a mock LiteLLMClient.
    We'll patch methods in tests if needed,
    or just return a default MagicMock-based approach.
    """
    client = LiteLLMClient(api_key="FAKE_KEY")  # real init
    # Overwrite internal method calls with MagicMocks
    client.get_embedding = MagicMock()
    client.call_structured = MagicMock()
    return client


def test_embed_node_if_needed_no_auto_embed(mock_db, mock_llm_client):
    """
    If auto_embed = False, embed_node_if_needed should do nothing.
    """
    integration = LLMToolIntegration(db=mock_db, llm_client=mock_llm_client, auto_embed=False)
    integration.embed_node_if_needed("Person_1")
    # get_embedding should not be called at all
    mock_llm_client.get_embedding.assert_not_called()


def test_embed_node_if_needed_success(mock_db, mock_llm_client):
    """
    If auto_embed=True and node has fields to embed, we call get_embedding
    and store the result in the node.
    """
    mock_llm_client.get_embedding.return_value = [0.42, 0.43, 0.44]
    integration = LLMToolIntegration(db=mock_db, llm_client=mock_llm_client, auto_embed=True)
    integration.embed_node_if_needed("Person_1")
    mock_llm_client.get_embedding.assert_called_once()


def test_auto_embed_new_nodes(mock_db, mock_llm_client):
    """
    auto_embed_new_nodes should embed all nodes that do not already have an embedding,
    if label_filter matches.
    """
    mock_llm_client.get_embedding.return_value = [0.9, 0.8, 0.7]
    integration = LLMToolIntegration(db=mock_db, llm_client=mock_llm_client, auto_embed=True)
    integration.auto_embed_new_nodes(label_filter="Person")

    # We expect DB query() to return 2 "Person" nodes:
    #   1) Person_1 => no embedding => embed
    #   2) Person_2 => has embedding => skip
    mock_llm_client.get_embedding.assert_called_once()


def test_structured_extraction_for_node_ok(mock_db, mock_llm_client):
    """
    structured_extraction_for_node should call call_structured,
    then create a new node with merged properties, then embed if auto_embed=True.
    """
    mock_llm_client.call_structured.return_value = {"age": 30, "nicknames": ["Aly"], "hobby": "cooking"}
    integration = LLMToolIntegration(db=mock_db, llm_client=mock_llm_client, auto_embed=True)
    node_id = integration.structured_extraction_for_node(
        text="She is 30, loves cooking and is sometimes called Aly.",
        schema={"age": 0, "nicknames": [], "hobby": ""},
        node_label="Person",
        default_properties={"country": "USA"}
    )
    assert node_id == "Person_123"  # from mock_db's create_node_impl

    # Check that call_structured was called
    mock_llm_client.call_structured.assert_called_once()
    mock_llm_client.get_embedding.assert_called_once()


def test_structured_extraction_for_node_error(mock_db, mock_llm_client):
    """
    If the call_structured call raises LiteLLMError, we should raise ValueError in the method.
    """
    mock_llm_client.call_structured.side_effect = LiteLLMError("Bad parse")

    integration = LLMToolIntegration(db=mock_db, llm_client=mock_llm_client, auto_embed=True)
    with pytest.raises(ValueError) as excinfo:
        integration.structured_extraction_for_node(
            text="some text",
            schema={"foo": "bar"},
            node_label="Person"
        )
    assert "LLM extraction error: Bad parse" in str(excinfo.value)
    mock_llm_client.get_embedding.assert_not_called()

================================================================================


================================================================================
FILE: docs/README.md
================================================================================
# GraphRouter Documentation

## Table of Contents

1. [Introduction](#introduction)
2. [Installation](#installation)
3. [Basic Usage](#basic-usage)
4. [Database Backends](#database-backends)
5. [Advanced Features](#advanced-features)
6. [API Reference](#api-reference)
7. [LLM Integration](#llm-integration)

## Introduction

GraphRouter is a flexible Python library that provides a unified interface for working with multiple graph database backends. It allows you to write database-agnostic code that can work with different graph databases without changing your application logic.

## Installation

```bash
pip install graphrouter
```

## Basic Usage

```python
from graphrouter import LocalGraphDatabase, Query

# Initialize database
db = LocalGraphDatabase()
db.connect(db_path="graph.json")

# Create nodes
alice = db.create_node('Person', {'name': 'Alice', 'age': 30})
bob = db.create_node('Person', {'name': 'Bob', 'age': 25})

# Create relationship
db.create_edge(alice, bob, 'FRIENDS_WITH', {'since': '2023'})

# Query the graph
query = Query()
query.filter(Query.label_equals('Person'))
results = db.query(query)
```

## Database Backends

### Local JSON Backend
Ideal for development and testing, stores data in a JSON file.

```python
from graphrouter import LocalGraphDatabase

db = LocalGraphDatabase()
db.connect(db_path="graph.json")
```

### Neo4j Backend
For production use with Neo4j database.

```python
from graphrouter import Neo4jGraphDatabase

db = Neo4jGraphDatabase()
db.connect(
    uri="bolt://localhost:7687",
    username="neo4j",
    password="password"
)
```

### FalkorDB Backend
For high-performance graph operations.

```python
from graphrouter import FalkorDBGraphDatabase

db = FalkorDBGraphDatabase()
db.connect(
    host="localhost",
    port=6379,
    password="optional-password",
    graph_name="my_graph"
)
```

## Advanced Features

### Ontology Management

```python
from graphrouter import Ontology

ontology = Ontology()
ontology.add_node_type(
    'Person',
    {'name': 'str', 'age': 'int'},
    required=['name']
)
ontology.add_edge_type(
    'KNOWS',
    {'since': 'str'},
    required=['since']
)

db.set_ontology(ontology)
```

### Complex Queries

```python
# Find paths
query = Query()
query.find_path(
    'Person', 'Company',
    ['WORKS_AT', 'SUBSIDIARY_OF'],
    min_depth=1,
    max_depth=3
)

# Aggregations
query = Query()
query.filter(Query.label_equals('Person'))
query.aggregate(AggregationType.AVG, 'age', 'avg_age')
```

### Transaction Management

```python
from graphrouter.transaction import transaction_scope

with transaction_scope(db) as tx:
    node1 = db.create_node('Person', {'name': 'Alice'})
    node2 = db.create_node('Person', {'name': 'Bob'})
    db.create_edge(node1, node2, 'FRIENDS_WITH', {'since': '2023'})
```

## API Reference

### GraphDatabase Base Class
The abstract base class that all database implementations extend.

#### Methods

- `connect(**kwargs) -> bool`: Connect to the database
- `disconnect() -> bool`: Disconnect from the database
- `create_node(label: str, properties: Dict[str, Any]) -> str`: Create a new node
- `get_node(node_id: str) -> Optional[Dict[str, Any]]`: Retrieve a node
- `update_node(node_id: str, properties: Dict[str, Any]) -> bool`: Update a node
- `delete_node(node_id: str) -> bool`: Delete a node
- `create_edge(from_id: str, to_id: str, label: str, properties: Optional[Dict[str, Any]] = None) -> str`: Create an edge
- `get_edge(edge_id: str) -> Optional[Dict[str, Any]]`: Retrieve an edge
- `update_edge(edge_id: str, properties: Dict[str, Any]) -> bool`: Update an edge
- `delete_edge(edge_id: str) -> bool`: Delete an edge
- `query(query: Query) -> List[Dict[str, Any]]`: Execute a query

### Query Builder
The Query class provides a fluent interface for building graph queries.

#### Methods

- `filter(condition: Callable) -> Query`: Add a node filter
- `filter_relationship(condition: Callable) -> Query`: Add a relationship filter
- `sort(key: str, reverse: bool = False) -> Query`: Sort results
- `limit_results(limit: int) -> Query`: Limit number of results
- `find_path(start_label: str, end_label: str, relationships: List[str], min_depth: Optional[int] = None, max_depth: Optional[int] = None) -> Query`: Find paths between nodes
- `aggregate(type: AggregationType, field: Optional[str] = None, alias: Optional[str] = None) -> Query`: Add aggregation

## LLM Integration

The system provides seamless integration with Large Language Models for:
- Flexible node property extraction per type
- Relationship identification with configurable rules
- Schema validation with ontology support
- Conditional extraction based on triggers
- Multi-node type processing
================================================================================


================================================================================
FILE: docs/advanced_usage.md
================================================================================
# Advanced Usage Guide

## Performance Optimization

### Connection Pooling

```python
# Initialize with custom pool size
db = Neo4jGraphDatabase(pool_size=10)

# Connection pool is automatically managed
db.connect(uri="bolt://localhost:7687", username="neo4j", password="password")
```

### Query Caching

```python
# Configure custom cache TTL
db._cache = QueryCache(ttl=600)  # 10 minutes

# Pattern-based cache invalidation
db._cache.invalidate("node:*")  # Invalidate all node caches
db._cache.invalidate("query:*")  # Invalidate all query caches
```

### Batch Operations

```python
# Efficient bulk loading
nodes = [
    {
        'label': 'Person',
        'properties': {'name': f'User{i}', 'age': 20 + i}
    }
    for i in range(1000)
]

# Single transaction for all nodes
node_ids = db.batch_create_nodes(nodes)

# Create relationships in bulk
edges = [
    {
        'from_id': node_ids[i],
        'to_id': node_ids[i+1],
        'label': 'KNOWS'
    }
    for i in range(len(node_ids)-1)
]

edge_ids = db.batch_create_edges(edges)
```

## Advanced Querying

### Complex Filters

```python
from graphrouter import Query

# Combine multiple filters
query = Query()
query.filter(Query.label_equals('Person'))
query.filter(Query.property_greater_than('age', 25))
query.filter(Query.property_contains('interests', 'coding'))
query.filter(
    lambda node: any(
        city in node['properties'].get('location', '')
        for city in ['New York', 'London', 'Tokyo']
    )
)
```

### Custom Query Builders

```python
class AdvancedQuery(Query):
    @staticmethod
    def age_range(min_age: int, max_age: int) -> Callable:
        def filter_func(node: Dict[str, Any]) -> bool:
            age = node['properties'].get('age', 0)
            return min_age <= age <= max_age
        return filter_func

    @staticmethod
    def has_complete_profile() -> Callable:
        required_fields = {'name', 'email', 'phone'}
        def filter_func(node: Dict[str, Any]) -> bool:
            return all(
                field in node['properties']
                for field in required_fields
            )
        return filter_func
```

## Schema Management

### Advanced Ontology

```python
from graphrouter import Ontology
from typing import List, Union

# Define complex schema
ontology = Ontology()

# Node types with nested structures
ontology.add_node_type('Person', {
    'name': str,
    'age': int,
    'contacts': List[str],
    'address': {
        'street': str,
        'city': str,
        'country': str
    }
})

# Edge types with validation
ontology.add_edge_type('KNOWS', {
    'since': str,
    'strength': float,
    'tags': List[str]
})

# Apply schema to database
db.set_ontology(ontology)
```

## Monitoring and Metrics

### Custom Metrics Collection

```python
# Get detailed operation statistics
detailed_metrics = db._monitor.get_detailed_metrics()

# Analyze specific operation
query_stats = db._monitor.get_operation_stats('query')
print(f"Query Statistics:")
print(f"Average duration: {query_stats['avg_duration']:.3f}s")
print(f"Median duration: {query_stats['median_duration']:.3f}s")
print(f"Standard deviation: {query_stats['std_dev']:.3f}s")
print(f"Error rate: {query_stats['error_rate']*100:.1f}%")

# Reset metrics for a fresh start
db._monitor.reset()
```

### Performance Profiling

```python
import time
from contextlib import contextmanager

@contextmanager
def profile_operation(db, operation_name: str):
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        db._monitor.record_operation(operation_name, duration)

# Use in code
with profile_operation(db, 'complex_query'):
    results = db.query(complex_query)
```

## Error Handling and Recovery

### Retry Logic

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class RetryingDatabase(Neo4jGraphDatabase):
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def query(self, query: Query) -> List[Dict[str, Any]]:
        return super().query(query)
```

### Transaction Management

```python
class TransactionManager:
    def __init__(self, db):
        self.db = db
        self.operations = []

    def add_operation(self, op_type: str, *args, **kwargs):
        self.operations.append((op_type, args, kwargs))

    def execute(self):
        results = []
        try:
            for op_type, args, kwargs in self.operations:
                if op_type == 'create_node':
                    results.append(
                        self.db.create_node(*args, **kwargs)
                    )
                elif op_type == 'create_edge':
                    results.append(
                        self.db.create_edge(*args, **kwargs)
                    )
            return results
        except Exception as e:
            # Implement rollback logic
            raise
```

## Best Practices

1. **Connection Management**
   - Use appropriate pool size for your workload
   - Implement connection health checks
   - Handle reconnection gracefully

2. **Query Optimization**
   - Use batch operations for bulk operations
   - Implement caching for read-heavy workloads
   - Monitor and optimize slow queries

3. **Error Handling**
   - Implement comprehensive error handling
   - Use retry logic for transient failures
   - Log errors with context for debugging

4. **Performance Monitoring**
   - Regular metric collection and analysis
   - Set up alerting for performance degradation
   - Monitor cache hit rates and query times

5. **Data Validation**
   - Define comprehensive schemas
   - Validate data before writing
   - Maintain data consistency

For more information, refer to the [API Reference](api_reference.md) and [Contribution Guidelines](../CONTRIBUTING.md).

================================================================================


================================================================================
FILE: docs/api_reference.md
================================================================================
# GraphRouter API Reference

## Core Classes

### GraphDatabase

The abstract base class that all database implementations inherit from.

```python
class GraphDatabase(ABC):
    def __init__(self, pool_size: int = 5):
        """Initialize the database connection.
        
        Args:
            pool_size: Size of the connection pool (default: 5)
        """

    def connect(self, **kwargs) -> bool:
        """Connect to the database.
        
        Returns:
            bool: True if connection successful
        
        Raises:
            ConnectionError: If connection fails
        """

    def query(self, query: Query) -> List[Dict[str, Any]]:
        """Execute a query and return results.
        
        Args:
            query: Query object containing filters and sort options
        
        Returns:
            List[Dict[str, Any]]: List of matching nodes
        """
```

### Query Builder

The Query class provides a fluent interface for building graph queries.

```python
class Query:
    def filter(self, filter_func: Callable[[Dict[str, Any]], bool]) -> 'Query':
        """Add a filter to the query.
        
        Args:
            filter_func: Function that takes a node and returns bool
        
        Returns:
            Query: The query object for chaining
        """
    
    @staticmethod
    def label_equals(label: str) -> Callable:
        """Create a filter that matches nodes with the given label."""
    
    @staticmethod
    def property_equals(prop: str, value: Any) -> Callable:
        """Create a filter that matches nodes where property equals value."""
```

### Cache Management

The QueryCache class handles caching of query results.

```python
class QueryCache:
    def __init__(self, ttl: int = 300):
        """Initialize the cache.
        
        Args:
            ttl: Time-to-live in seconds for cache entries (default: 300)
        """
```

### Performance Monitoring

The PerformanceMonitor class tracks operation metrics.

```python
class PerformanceMonitor:
    def get_average_times(self) -> Dict[str, float]:
        """Get average execution time for each operation type."""
    
    def get_detailed_metrics(self) -> Dict[str, Dict[str, float]]:
        """Get detailed metrics for all operations."""
```

## Database Implementations

### Neo4j Backend

```python
class Neo4jGraphDatabase(GraphDatabase):
    def connect(self, uri: str, username: str, password: str) -> bool:
        """Connect to Neo4j database.
        
        Args:
            uri: Neo4j connection URI (e.g., 'bolt://localhost:7687')
            username: Neo4j username
            password: Neo4j password
        """
```

### FalkorDB Backend

```python
class FalkorDBGraphDatabase(GraphDatabase):
    def connect(self, **kwargs) -> bool:
        """Connect to FalkorDB.
        
        Args:
            host: Redis host (default: 'localhost')
            port: Redis port (default: 6379)
            password: Redis password
            graph_name: Name of the graph (default: 'graph')
        """
```

## Error Handling

The library provides several custom exception classes:

- `ConnectionError`: Raised when database connection fails
- `QueryError`: Raised when query execution fails
- `ValidationError`: Raised when data validation fails

## Usage Examples

See the [Quick Start Guide](quickstart.md) for usage examples.

================================================================================


================================================================================
FILE: docs/installation.md
================================================================================
# Installation Guide

## Requirements

- Python 3.8 or higher
- pip package manager

## Basic Installation

Install GraphRouter using pip:

```bash
pip install graphrouter
```

This installs the core package with basic functionality.

## Installing with Backend Support

### Neo4j Support

```bash
pip install 'graphrouter[neo4j]'
```

### FalkorDB Support

```bash
pip install 'graphrouter[falkordb]'
```

### Complete Installation

To install with all optional dependencies:

```bash
pip install 'graphrouter[all]'
```

## Development Installation

For contributing to GraphRouter:

1. Clone the repository:
   ```bash
   git clone https://github.com/graphrouter/graphrouter.git
   cd graphrouter
   ```

2. Install development dependencies:
   ```bash
   pip install -e ".[dev]"
   ```

3. Install test dependencies:
   ```bash
   pip install -e ".[test]"
   ```

## Backend Setup

### Neo4j Setup

1. [Install Neo4j](https://neo4j.com/docs/operations-manual/current/installation/)
2. Start Neo4j server:
   ```bash
   neo4j start
   ```
3. Configure connection in your code:
   ```python
   from graphrouter import Neo4jGraphDatabase
   
   db = Neo4jGraphDatabase()
   db.connect(
       uri="bolt://localhost:7687",
       username="neo4j",
       password="your-password"
   )
   ```

### FalkorDB Setup

1. [Install Redis](https://redis.io/docs/getting-started/)
2. Install FalkorDB module:
   ```bash
   redis-cli module load falkordb.so
   ```
3. Configure connection in your code:
   ```python
   from graphrouter import FalkorDBGraphDatabase
   
   db = FalkorDBGraphDatabase()
   db.connect(
       host="localhost",
       port=6379,
       password="your-password"
   )
   ```

## Troubleshooting

### Common Issues

1. **Connection Errors**
   - Verify database server is running
   - Check connection credentials
   - Ensure proper network access

2. **Import Errors**
   - Verify Python version compatibility
   - Check all required dependencies are installed
   - Use `pip list` to verify package installation

### Getting Help

- Check the [FAQ](faq.md) for common questions
- Report issues on [GitHub](https://github.com/graphrouter/graphrouter/issues)
- Join our [Discord community](https://discord.gg/graphrouter)

## Next Steps

- Follow the [Quick Start Guide](quickstart.md)
- Read the [API Reference](api_reference.md)
- Learn from [Advanced Usage Examples](advanced_usage.md)

================================================================================


================================================================================
FILE: docs/quickstart.md
================================================================================
# Quick Start Guide

This guide will help you get started with GraphRouter quickly.

## Basic Usage

### 1. Installation

```bash
pip install graphrouter
```

### 2. Creating a Database Connection

```python
from graphrouter import Neo4jGraphDatabase

# Initialize database
db = Neo4jGraphDatabase()
db.connect(
    uri="bolt://localhost:7687",
    username="neo4j",
    password="your-password"
)
```

### 3. Creating Nodes and Relationships

```python
# Create nodes
alice = db.create_node('Person', {
    'name': 'Alice',
    'age': 30,
    'occupation': 'Engineer'
})

bob = db.create_node('Person', {
    'name': 'Bob',
    'age': 28,
    'occupation': 'Designer'
})

# Create a relationship
db.create_edge(alice, bob, 'KNOWS', {
    'since': '2023-01-01',
    'type': 'Colleague'
})
```

### 4. Querying the Graph

```python
from graphrouter import Query

# Find all engineers
query = Query()
query.filter(Query.label_equals('Person'))
query.filter(Query.property_equals('occupation', 'Engineer'))

results = db.query(query)
for node in results:
    print(f"Found engineer: {node['properties']['name']}")
```

## Advanced Features

### 1. Using Query Cache

```python
# Cache is enabled by default (TTL: 300 seconds)
cached_results = db.query(query)  # First query hits database
same_results = db.query(query)    # Second query uses cache

# Clear cache if needed
db.clear_cache()
```

### 2. Monitoring Performance

```python
# Get performance metrics
metrics = db.get_performance_metrics()

print("Operation Statistics:")
for operation, stats in metrics.items():
    print(f"\n{operation}:")
    print(f"Average duration: {stats['avg_duration']:.3f}s")
    print(f"Success rate: {(1 - stats['error_rate']) * 100:.1f}%")
```

### 3. Batch Operations

```python
# Batch create nodes
nodes = [
    {'label': 'Person', 'properties': {'name': 'Carol', 'age': 25}},
    {'label': 'Person', 'properties': {'name': 'Dave', 'age': 32}},
]
node_ids = db.batch_create_nodes(nodes)

# Batch create relationships
edges = [
    {'from_id': node_ids[0], 'to_id': node_ids[1], 'label': 'KNOWS'},
]
edge_ids = db.batch_create_edges(edges)
```

### 4. Error Handling

```python
from graphrouter.errors import ConnectionError, QueryError

try:
    results = db.query(query)
except ConnectionError as e:
    print(f"Connection failed: {e}")
except QueryError as e:
    print(f"Query failed: {e}")
```

## Next Steps

- Read the [API Reference](api_reference.md) for detailed documentation
- Check [Advanced Usage](advanced_usage.md) for more features
- Learn about [Schema Validation](schema_validation.md)

## Common Patterns

### 1. Complex Queries

```python
# Find people over 25 who know engineers
query = Query()
query.filter(Query.label_equals('Person'))
query.filter(Query.property_greater_than('age', 25))
query.sort('age', reverse=True)
query.limit_results(5)

results = db.query(query)
```

### 2. Transaction Management

```python
# Using transaction context manager
with db.transaction() as tx:
    node_id = tx.create_node('Person', {'name': 'Eve'})
    tx.create_edge(node_id, other_id, 'KNOWS')
```

### 3. Schema Validation

```python
from graphrouter import Ontology

# Define schema
ontology = Ontology()
ontology.add_node_type('Person', {
    'name': str,
    'age': int,
    'email': str
})

# Apply schema
db.set_ontology(ontology)
```

## Tips and Best Practices

1. Always use connection pooling for production
2. Enable caching for read-heavy workloads
3. Use batch operations for bulk data loading
4. Monitor performance metrics in production
5. Implement proper error handling

For more detailed examples and advanced usage, refer to the [Advanced Guide](advanced_usage.md).

================================================================================
